{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Lesson 2: From Backpropagation to Modern AI\n",
    "\n",
    "## Understanding How Neural Networks Learn and Scale\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand backpropagation: the algorithm that powers neural network training\n",
    "- Explore gradient descent and optimization techniques\n",
    "- Learn about modern architectures: CNNs, RNNs, Transformers\n",
    "- Understand the principles behind Large Language Models (LLMs)\n",
    "- See how we got from XOR to ChatGPT\n",
    "\n",
    "**Duration:** ~90 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: The Math Behind Learning - Backpropagation\n",
    "\n",
    "In Lessons 1A and 1B, you saw neural networks learn. But **how** do they actually adjust their weights?\n",
    "\n",
    "The answer: **Backpropagation** (backward propagation of errors)\n",
    "\n",
    "### The Big Idea:\n",
    "\n",
    "1. **Forward Pass**: Input flows through network ‚Üí produces prediction\n",
    "2. **Calculate Loss**: Compare prediction to true answer\n",
    "3. **Backward Pass**: Calculate how much each weight contributed to the error\n",
    "4. **Update Weights**: Adjust weights to reduce the error\n",
    "5. **Repeat**: Do this millions of times\n",
    "\n",
    "### Mathematical Foundation: The Chain Rule\n",
    "\n",
    "Backpropagation is just calculus's **chain rule** applied recursively through the network:\n",
    "\n",
    "```\n",
    "‚àÇLoss/‚àÇw = ‚àÇLoss/‚àÇoutput √ó ‚àÇoutput/‚àÇw\n",
    "```\n",
    "\n",
    "This tells us: \"How does changing weight w affect the final loss?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")\n",
    "print(\"\\nüìö Topics we'll cover:\")\n",
    "print(\"  1. Backpropagation algorithm\")\n",
    "print(\"  2. Gradient descent variants\")\n",
    "print(\"  3. Optimization techniques\")\n",
    "print(\"  4. Modern architectures (CNNs, RNNs, Transformers)\")\n",
    "print(\"  5. Large Language Models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualizing Gradient Descent\n",
    "\n",
    "Gradient descent is the optimization algorithm that uses backpropagation's gradients to update weights.\n",
    "\n",
    "**Intuition**: Imagine you're blindfolded on a mountain and want to reach the valley:\n",
    "1. Feel the slope under your feet (compute gradient)\n",
    "2. Take a step downhill (update weights)\n",
    "3. Repeat until you can't go lower (convergence)\n",
    "\n",
    "**Formula**: w_new = w_old - learning_rate √ó gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient descent on a simple 2D function\n",
    "def f(x, y):\n",
    "    \"\"\"Simple loss function: bowl-shaped\"\"\"\n",
    "    return (x - 2)**2 + (y - 1)**2\n",
    "\n",
    "def gradient_f(x, y):\n",
    "    \"\"\"Gradient of the loss function\"\"\"\n",
    "    dx = 2 * (x - 2)\n",
    "    dy = 2 * (y - 1)\n",
    "    return dx, dy\n",
    "\n",
    "# Create 3D surface\n",
    "x_range = np.linspace(-1, 5, 100)\n",
    "y_range = np.linspace(-2, 4, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = f(X, Y)\n",
    "\n",
    "# Run gradient descent\n",
    "def gradient_descent(start_x, start_y, learning_rate=0.1, num_steps=50):\n",
    "    \"\"\"Perform gradient descent\"\"\"\n",
    "    path = [(start_x, start_y)]\n",
    "    x, y = start_x, start_y\n",
    "    \n",
    "    for _ in range(num_steps):\n",
    "        dx, dy = gradient_f(x, y)\n",
    "        x = x - learning_rate * dx\n",
    "        y = y - learning_rate * dy\n",
    "        path.append((x, y))\n",
    "    \n",
    "    return np.array(path)\n",
    "\n",
    "# Compare different learning rates\n",
    "paths = {\n",
    "    'Too Small (LR=0.01)': gradient_descent(-1, -2, learning_rate=0.01, num_steps=100),\n",
    "    'Just Right (LR=0.1)': gradient_descent(-1, -2, learning_rate=0.1, num_steps=50),\n",
    "    'Too Large (LR=0.5)': gradient_descent(-1, -2, learning_rate=0.5, num_steps=50)\n",
    "}\n",
    "\n",
    "# Visualize\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "\n",
    "# 3D surface\n",
    "ax1 = fig.add_subplot(131, projection='3d')\n",
    "ax1.plot_surface(X, Y, Z, alpha=0.6, cmap='viridis')\n",
    "ax1.set_xlabel('Weight 1')\n",
    "ax1.set_ylabel('Weight 2')\n",
    "ax1.set_zlabel('Loss')\n",
    "ax1.set_title('Loss Surface (3D)', fontweight='bold')\n",
    "\n",
    "# Contour plot with paths\n",
    "ax2 = fig.add_subplot(132)\n",
    "contour = ax2.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.6)\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "\n",
    "colors = ['blue', 'green', 'red']\n",
    "for (label, path), color in zip(paths.items(), colors):\n",
    "    ax2.plot(path[:, 0], path[:, 1], 'o-', color=color, label=label, linewidth=2, markersize=4)\n",
    "    ax2.plot(path[0, 0], path[0, 1], 'k*', markersize=15, label='Start' if color == 'blue' else '')\n",
    "\n",
    "ax2.plot(2, 1, 'r*', markersize=20, label='Optimum')\n",
    "ax2.set_xlabel('Weight 1')\n",
    "ax2.set_ylabel('Weight 2')\n",
    "ax2.set_title('Gradient Descent Paths', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss over iterations\n",
    "ax3 = fig.add_subplot(133)\n",
    "for (label, path), color in zip(paths.items(), colors):\n",
    "    losses = [f(x, y) for x, y in path]\n",
    "    ax3.plot(losses, color=color, label=label, linewidth=2)\n",
    "\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.set_title('Loss Over Time', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Key Observations:\")\n",
    "print(\"  ‚Ä¢ Learning rate too small ‚Üí slow convergence (many iterations needed)\")\n",
    "print(\"  ‚Ä¢ Learning rate just right ‚Üí smooth, efficient convergence\")\n",
    "print(\"  ‚Ä¢ Learning rate too large ‚Üí oscillation and instability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Backpropagation Step-by-Step\n",
    "\n",
    "Let's see backpropagation in action on a simple network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2-layer network with detailed backprop\n",
    "class DetailedBackpropNetwork:\n",
    "    def __init__(self):\n",
    "        \"\"\"Tiny network: 2 inputs ‚Üí 2 hidden ‚Üí 1 output\"\"\"\n",
    "        # Initialize small weights for visualization\n",
    "        self.W1 = np.array([[0.5, 0.3],   # Input ‚Üí Hidden\n",
    "                           [0.2, 0.8]])\n",
    "        self.b1 = np.array([[0.1, 0.2]])\n",
    "        \n",
    "        self.W2 = np.array([[0.4],        # Hidden ‚Üí Output\n",
    "                           [0.6]])\n",
    "        self.b2 = np.array([[0.3]])\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_detailed(self, X):\n",
    "        \"\"\"Forward pass with detailed intermediate values\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FORWARD PASS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nüì• Input: {X}\")\n",
    "        \n",
    "        # Layer 1\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        print(f\"\\nüî∑ Hidden layer (before activation): {self.z1}\")\n",
    "        print(f\"üî∑ Hidden layer (after sigmoid): {self.a1}\")\n",
    "        \n",
    "        # Layer 2\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        print(f\"\\nüì§ Output (before activation): {self.z2}\")\n",
    "        print(f\"üì§ Output (after sigmoid): {self.a2}\")\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def backward_detailed(self, X, y, learning_rate=0.5):\n",
    "        \"\"\"Backward pass with detailed gradient calculations\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"BACKWARD PASS (Backpropagation)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Output layer error\n",
    "        error = y - self.a2\n",
    "        print(f\"\\n‚ùå Error (target - prediction): {error}\")\n",
    "        print(f\"‚ùå Loss (MSE): {np.mean(error**2):.6f}\")\n",
    "        \n",
    "        # Output layer gradients\n",
    "        delta_output = error * self.sigmoid_derivative(self.a2)\n",
    "        print(f\"\\nüî∫ Output layer delta: {delta_output}\")\n",
    "        \n",
    "        # Hidden layer error (backpropagate)\n",
    "        hidden_error = delta_output.dot(self.W2.T)\n",
    "        delta_hidden = hidden_error * self.sigmoid_derivative(self.a1)\n",
    "        print(f\"\\nüî∫ Hidden layer delta: {delta_hidden}\")\n",
    "        \n",
    "        # Calculate weight updates\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(\"WEIGHT UPDATES\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        dW2 = self.a1.T.dot(delta_output)\n",
    "        db2 = np.sum(delta_output, axis=0, keepdims=True)\n",
    "        dW1 = X.T.dot(delta_hidden)\n",
    "        db1 = np.sum(delta_hidden, axis=0, keepdims=True)\n",
    "        \n",
    "        print(f\"\\nGradients for W2 (hidden‚Üíoutput weights):\\n{dW2}\")\n",
    "        print(f\"\\nGradients for W1 (input‚Üíhidden weights):\\n{dW1}\")\n",
    "        \n",
    "        # Update weights\n",
    "        self.W2 += learning_rate * dW2\n",
    "        self.b2 += learning_rate * db2\n",
    "        self.W1 += learning_rate * dW1\n",
    "        self.b1 += learning_rate * db1\n",
    "        \n",
    "        print(f\"\\n‚úÖ Weights updated with learning rate {learning_rate}\")\n",
    "\n",
    "# Demonstrate one training step\n",
    "net = DetailedBackpropNetwork()\n",
    "\n",
    "# Simple XOR-like example\n",
    "X = np.array([[1, 0]])\n",
    "y = np.array([[1]])\n",
    "\n",
    "print(\"\\nüéì BACKPROPAGATION WALKTHROUGH\")\n",
    "print(\"Training example: Input [1, 0] ‚Üí Target 1\")\n",
    "\n",
    "# Before training\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"# BEFORE TRAINING\")\n",
    "print(\"#\"*60)\n",
    "output_before = net.forward_detailed(X)\n",
    "\n",
    "# One training step\n",
    "net.backward_detailed(X, y, learning_rate=0.5)\n",
    "\n",
    "# After training\n",
    "print(\"\\n\" + \"#\"*60)\n",
    "print(\"# AFTER ONE TRAINING STEP\")\n",
    "print(\"#\"*60)\n",
    "output_after = net.forward_detailed(X)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTarget output: {y[0][0]}\")\n",
    "print(f\"Output before training: {output_before[0][0]:.6f}\")\n",
    "print(f\"Output after training:  {output_after[0][0]:.6f}\")\n",
    "print(f\"\\n‚ú® The network got closer to the target!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Advanced Optimization Techniques\n",
    "\n",
    "Modern neural networks don't use simple gradient descent. They use sophisticated optimizers:\n",
    "\n",
    "### 1. **Momentum**\n",
    "- Remembers previous gradients\n",
    "- Helps accelerate in consistent directions\n",
    "- Reduces oscillation\n",
    "\n",
    "### 2. **Adam (Adaptive Moment Estimation)**\n",
    "- Combines momentum with adaptive learning rates\n",
    "- Most popular optimizer for deep learning\n",
    "- Default choice for most tasks\n",
    "\n",
    "### 3. **Learning Rate Schedules**\n",
    "- Start with large learning rate ‚Üí fast initial progress\n",
    "- Gradually decrease ‚Üí fine-tuning\n",
    "- Common: step decay, exponential decay, cosine annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare optimization algorithms\n",
    "def optimize_comparison():\n",
    "    \"\"\"Compare different optimizers on the same problem\"\"\"\n",
    "    \n",
    "    # Same 2D function as before\n",
    "    def f(x, y):\n",
    "        return (x - 2)**2 + (y - 1)**2\n",
    "    \n",
    "    def grad_f(x, y):\n",
    "        return 2*(x-2), 2*(y-1)\n",
    "    \n",
    "    # Standard Gradient Descent\n",
    "    def gradient_descent(start, lr=0.1, steps=50):\n",
    "        x, y = start\n",
    "        path = [(x, y)]\n",
    "        for _ in range(steps):\n",
    "            dx, dy = grad_f(x, y)\n",
    "            x -= lr * dx\n",
    "            y -= lr * dy\n",
    "            path.append((x, y))\n",
    "        return np.array(path)\n",
    "    \n",
    "    # Gradient Descent with Momentum\n",
    "    def momentum(start, lr=0.01, momentum=0.9, steps=50):\n",
    "        x, y = start\n",
    "        vx, vy = 0, 0\n",
    "        path = [(x, y)]\n",
    "        for _ in range(steps):\n",
    "            dx, dy = grad_f(x, y)\n",
    "            vx = momentum * vx + lr * dx\n",
    "            vy = momentum * vy + lr * dy\n",
    "            x -= vx\n",
    "            y -= vy\n",
    "            path.append((x, y))\n",
    "        return np.array(path)\n",
    "    \n",
    "    # Simplified Adam\n",
    "    def adam(start, lr=0.1, beta1=0.9, beta2=0.999, steps=50):\n",
    "        x, y = start\n",
    "        mx, my = 0, 0\n",
    "        vx, vy = 0, 0\n",
    "        path = [(x, y)]\n",
    "        eps = 1e-8\n",
    "        \n",
    "        for t in range(1, steps+1):\n",
    "            dx, dy = grad_f(x, y)\n",
    "            \n",
    "            # Update biased first moment\n",
    "            mx = beta1 * mx + (1 - beta1) * dx\n",
    "            my = beta1 * my + (1 - beta1) * dy\n",
    "            \n",
    "            # Update biased second moment\n",
    "            vx = beta2 * vx + (1 - beta2) * dx**2\n",
    "            vy = beta2 * vy + (1 - beta2) * dy**2\n",
    "            \n",
    "            # Bias correction\n",
    "            mx_hat = mx / (1 - beta1**t)\n",
    "            my_hat = my / (1 - beta1**t)\n",
    "            vx_hat = vx / (1 - beta2**t)\n",
    "            vy_hat = vy / (1 - beta2**t)\n",
    "            \n",
    "            # Update parameters\n",
    "            x -= lr * mx_hat / (np.sqrt(vx_hat) + eps)\n",
    "            y -= lr * my_hat / (np.sqrt(vy_hat) + eps)\n",
    "            path.append((x, y))\n",
    "        \n",
    "        return np.array(path)\n",
    "    \n",
    "    # Run optimizers\n",
    "    start = (-1, -2)\n",
    "    paths = {\n",
    "        'Standard GD': gradient_descent(start, lr=0.1, steps=50),\n",
    "        'Momentum': momentum(start, lr=0.01, momentum=0.9, steps=50),\n",
    "        'Adam': adam(start, lr=0.1, steps=50)\n",
    "    }\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Paths\n",
    "    x_range = np.linspace(-1.5, 3, 100)\n",
    "    y_range = np.linspace(-2.5, 2, 100)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "    Z = f(X, Y)\n",
    "    \n",
    "    axes[0].contour(X, Y, Z, levels=20, alpha=0.3, cmap='viridis')\n",
    "    colors = ['blue', 'green', 'red']\n",
    "    for (name, path), color in zip(paths.items(), colors):\n",
    "        axes[0].plot(path[:, 0], path[:, 1], 'o-', color=color, label=name, linewidth=2, markersize=3)\n",
    "    \n",
    "    axes[0].plot(2, 1, 'r*', markersize=20, label='Optimum')\n",
    "    axes[0].set_xlabel('Parameter 1', fontsize=12)\n",
    "    axes[0].set_ylabel('Parameter 2', fontsize=12)\n",
    "    axes[0].set_title('Optimizer Paths', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss over time\n",
    "    for (name, path), color in zip(paths.items(), colors):\n",
    "        losses = [f(x, y) for x, y in path]\n",
    "        axes[1].plot(losses, color=color, label=name, linewidth=2)\n",
    "    \n",
    "    axes[1].set_xlabel('Iteration', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title('Convergence Speed', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "optimize_comparison()\n",
    "\n",
    "print(\"\\nüöÄ Optimizer Comparison:\")\n",
    "print(\"  ‚Ä¢ Standard GD: Simple but can be slow\")\n",
    "print(\"  ‚Ä¢ Momentum: Faster, smoother convergence\")\n",
    "print(\"  ‚Ä¢ Adam: Adaptive, robust, most popular for deep learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: From Simple Networks to Modern Architectures\n",
    "\n",
    "The principles you've learned (layers, activations, backpropagation) scale to modern AI systems:\n",
    "\n",
    "### Evolution of Neural Network Architectures:\n",
    "\n",
    "```\n",
    "1958: Perceptron (single layer)\n",
    "  ‚Üì\n",
    "1986: Multi-layer Perceptrons (backpropagation)\n",
    "  ‚Üì\n",
    "1998: Convolutional Neural Networks (CNNs) - for images\n",
    "  ‚Üì\n",
    "1997: Long Short-Term Memory (LSTM) - for sequences\n",
    "  ‚Üì\n",
    "2017: Transformers - for language and everything else\n",
    "  ‚Üì\n",
    "2020s: Large Language Models (GPT, Claude, etc.)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "**Problem**: Fully-connected networks (like our MNIST network) don't understand spatial relationships in images.\n",
    "\n",
    "**Solution**: CNNs use **convolutional layers** that scan across images with small filters.\n",
    "\n",
    "### How CNNs Work:\n",
    "\n",
    "1. **Convolutional Layers**: Learn local patterns (edges, textures)\n",
    "2. **Pooling Layers**: Reduce size while keeping important features\n",
    "3. **Fully Connected Layers**: Make final classification\n",
    "\n",
    "**Applications**:\n",
    "- Image classification (cats vs dogs)\n",
    "- Object detection (self-driving cars)\n",
    "- Face recognition\n",
    "- Medical image analysis\n",
    "\n",
    "**Famous CNNs**: AlexNet (2012), VGG, ResNet, EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize what a convolutional filter does\n",
    "def demonstrate_convolution():\n",
    "    # Create a simple image\n",
    "    image = np.zeros((10, 10))\n",
    "    image[4:7, :] = 1  # Horizontal line\n",
    "    \n",
    "    # Define filters\n",
    "    horizontal_filter = np.array([[-1, -1, -1],\n",
    "                                 [ 2,  2,  2],\n",
    "                                 [-1, -1, -1]])\n",
    "    \n",
    "    vertical_filter = np.array([[-1, 2, -1],\n",
    "                               [-1, 2, -1],\n",
    "                               [-1, 2, -1]])\n",
    "    \n",
    "    # Simple convolution\n",
    "    def convolve(img, kernel):\n",
    "        k_size = kernel.shape[0]\n",
    "        result = np.zeros_like(img)\n",
    "        pad = k_size // 2\n",
    "        \n",
    "        for i in range(pad, img.shape[0] - pad):\n",
    "            for j in range(pad, img.shape[1] - pad):\n",
    "                region = img[i-pad:i+pad+1, j-pad:j+pad+1]\n",
    "                result[i, j] = np.sum(region * kernel)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    h_response = convolve(image, horizontal_filter)\n",
    "    v_response = convolve(image, vertical_filter)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].imshow(image, cmap='gray')\n",
    "    axes[0].set_title('Original Image\\n(Horizontal Line)', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(h_response, cmap='RdBu')\n",
    "    axes[1].set_title('Horizontal Edge Detector\\n(Strong Response!)', fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(v_response, cmap='RdBu')\n",
    "    axes[2].set_title('Vertical Edge Detector\\n(Weak Response)', fontsize=14, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_convolution()\n",
    "\n",
    "print(\"\\nüîç Convolutional Filters:\")\n",
    "print(\"  ‚Ä¢ Each filter learns to detect a specific pattern\")\n",
    "print(\"  ‚Ä¢ Early layers: edges and simple shapes\")\n",
    "print(\"  ‚Ä¢ Deeper layers: complex patterns (faces, objects)\")\n",
    "print(\"  ‚Ä¢ CNNs can have hundreds of different filters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Recurrent Neural Networks (RNNs) & LSTMs\n",
    "\n",
    "**Problem**: Fully-connected and CNNs can't remember previous inputs.\n",
    "\n",
    "**Solution**: RNNs maintain a **hidden state** that carries information across time steps.\n",
    "\n",
    "### How RNNs Work:\n",
    "\n",
    "```\n",
    "Time:     t=0        t=1        t=2\n",
    "Input:    \"The\"  ‚Üí   \"cat\"  ‚Üí   \"sat\"\n",
    "          ‚Üì          ‚Üì          ‚Üì\n",
    "RNN:    [State] ‚Üí [State] ‚Üí [State]\n",
    "          ‚Üì          ‚Üì          ‚Üì\n",
    "Output:  next?      next?      next?\n",
    "```\n",
    "\n",
    "**Applications**:\n",
    "- Language modeling\n",
    "- Machine translation\n",
    "- Speech recognition\n",
    "- Time series prediction\n",
    "\n",
    "**Problem with RNNs**: Vanishing gradients (can't remember long sequences)\n",
    "\n",
    "**Solution**: LSTM (Long Short-Term Memory) with special gates to control what to remember/forget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Transformers - The Architecture That Changed Everything\n",
    "\n",
    "In 2017, the paper \"Attention Is All You Need\" introduced **Transformers**, revolutionizing AI.\n",
    "\n",
    "### Why Transformers Matter:\n",
    "\n",
    "**Before (RNNs)**:\n",
    "- Process sequences step-by-step (slow)\n",
    "- Struggle with long-range dependencies\n",
    "- Can't parallelize training\n",
    "\n",
    "**After (Transformers)**:\n",
    "- Process entire sequences at once (fast!)\n",
    "- Attention mechanism handles long-range dependencies\n",
    "- Highly parallelizable ‚Üí train on massive datasets\n",
    "\n",
    "### The Attention Mechanism:\n",
    "\n",
    "**Core Idea**: When processing a word, look at ALL other words and decide which ones are important.\n",
    "\n",
    "```\n",
    "Sentence: \"The cat sat on the mat\"\n",
    "\n",
    "Processing \"sat\":\n",
    "  - Pay attention to \"cat\" (who sat?)\n",
    "  - Pay attention to \"mat\" (where?)\n",
    "  - Less attention to \"the\" (less relevant)\n",
    "```\n",
    "\n",
    "### Self-Attention Formula:\n",
    "\n",
    "```\n",
    "Attention(Q, K, V) = softmax(QK^T / ‚àöd) V\n",
    "\n",
    "Where:\n",
    "Q = Queries (what am I looking for?)\n",
    "K = Keys (what do I contain?)\n",
    "V = Values (what information do I have?)\n",
    "```\n",
    "\n",
    "### Transformer Architecture:\n",
    "\n",
    "1. **Input Embedding**: Convert tokens to vectors\n",
    "2. **Positional Encoding**: Add position information\n",
    "3. **Multi-Head Attention**: Look at different relationships simultaneously\n",
    "4. **Feed-Forward Networks**: Process attention outputs\n",
    "5. **Layer Normalization**: Stabilize training\n",
    "6. **Residual Connections**: Help gradients flow\n",
    "\n",
    "**Stack many layers** (12, 24, 96+ layers) for more power!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified attention mechanism demonstration\n",
    "def simple_attention_demo():\n",
    "    # Sentence: \"The cat sat\"\n",
    "    tokens = ['The', 'cat', 'sat']\n",
    "    \n",
    "    # Simplified embeddings (in reality, these are learned)\n",
    "    # Each token ‚Üí 4D vector\n",
    "    embeddings = np.array([\n",
    "        [0.1, 0.3, 0.2, 0.1],  # The\n",
    "        [0.8, 0.2, 0.6, 0.9],  # cat\n",
    "        [0.5, 0.7, 0.3, 0.4]   # sat\n",
    "    ])\n",
    "    \n",
    "    # Compute attention scores (simplified)\n",
    "    # For each token, how much should it attend to other tokens?\n",
    "    scores = np.dot(embeddings, embeddings.T)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    def softmax(x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    attention_weights = softmax(scores)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Attention matrix\n",
    "    im = axes[0].imshow(attention_weights, cmap='YlOrRd', vmin=0, vmax=1)\n",
    "    axes[0].set_xticks(range(len(tokens)))\n",
    "    axes[0].set_yticks(range(len(tokens)))\n",
    "    axes[0].set_xticklabels(tokens, fontsize=12)\n",
    "    axes[0].set_yticklabels(tokens, fontsize=12)\n",
    "    axes[0].set_xlabel('Attending TO', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_ylabel('Attending FROM', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_title('Attention Matrix\\n(Darker = More Attention)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add values\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            text = axes[0].text(j, i, f'{attention_weights[i, j]:.2f}',\n",
    "                              ha=\"center\", va=\"center\", color=\"black\", fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im, ax=axes[0])\n",
    "    \n",
    "    # Attention for \"sat\"\n",
    "    axes[1].bar(tokens, attention_weights[2], color=['lightblue', 'orange', 'lightgreen'])\n",
    "    axes[1].set_ylabel('Attention Weight', fontsize=12)\n",
    "    axes[1].set_title('What does \"sat\" attend to?', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for i, (token, weight) in enumerate(zip(tokens, attention_weights[2])):\n",
    "        axes[1].text(i, weight + 0.02, f'{weight:.2f}', ha='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüéØ Attention Interpretation:\")\n",
    "    print(\"\\nWhen processing 'sat':\")\n",
    "    for token, weight in zip(tokens, attention_weights[2]):\n",
    "        print(f\"  ‚Ä¢ Attention to '{token}': {weight:.2f} ({weight*100:.0f}%)\")\n",
    "\n",
    "simple_attention_demo()\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"  Transformers learn WHAT to pay attention to during training.\")\n",
    "print(\"  This allows them to capture complex relationships in data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Large Language Models (LLMs)\n",
    "\n",
    "Modern AI assistants like ChatGPT, Claude, and others are **Large Language Models** built on Transformers.\n",
    "\n",
    "### What Makes Them \"Large\"?\n",
    "\n",
    "| Model | Parameters | Training Data |\n",
    "|-------|-----------|---------------|\n",
    "| GPT-2 (2019) | 1.5B | 40GB text |\n",
    "| GPT-3 (2020) | 175B | 570GB text |\n",
    "| GPT-4 (2023) | ~1.7T* | Massive scale |\n",
    "| Claude (Anthropic) | Unknown | Massive scale |\n",
    "\n",
    "*Estimated\n",
    "\n",
    "**Comparison**: \n",
    "- Your MNIST network: ~100K parameters\n",
    "- GPT-4: ~1,700,000,000,000 parameters (17 million times larger!)\n",
    "\n",
    "### How LLMs Work:\n",
    "\n",
    "1. **Pretraining**: Learn language patterns from massive text datasets\n",
    "   - Objective: Predict next token\n",
    "   - \"The cat sat on the ___\" ‚Üí model learns to predict \"mat\", \"floor\", \"chair\"\n",
    "\n",
    "2. **Fine-tuning**: Adapt for specific tasks\n",
    "   - Instruction following\n",
    "   - Question answering\n",
    "   - Code generation\n",
    "\n",
    "3. **Reinforcement Learning from Human Feedback (RLHF)**:\n",
    "   - Humans rank model outputs\n",
    "   - Model learns preferences\n",
    "   - Becomes more helpful, honest, harmless\n",
    "\n",
    "### Capabilities:\n",
    "\n",
    "- **Text generation**: Stories, essays, code\n",
    "- **Translation**: 100+ languages\n",
    "- **Reasoning**: Math, logic, common sense\n",
    "- **Coding**: Multiple programming languages\n",
    "- **Multimodal**: Text + images (GPT-4V, Claude 3)\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- Can generate plausible but incorrect information (\"hallucinations\")\n",
    "- Knowledge cutoff date (training data ends at specific time)\n",
    "- Can be misled by prompt engineering\n",
    "- Computationally expensive to run\n",
    "- Lack true understanding (statistical patterns, not consciousness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: From XOR to ChatGPT - The Journey\n",
    "\n",
    "Let's recap how we got from simple networks to modern AI:\n",
    "\n",
    "### 1. **Lesson 1A: XOR (2 ‚Üí 2 ‚Üí 1 network)**\n",
    "- 9 parameters\n",
    "- Solved non-linear classification\n",
    "- Proved multi-layer networks work\n",
    "\n",
    "### 2. **Lesson 1B: MNIST (784 ‚Üí 128 ‚Üí 10 network)**\n",
    "- ~100K parameters\n",
    "- Real-world image classification\n",
    "- 95%+ accuracy on handwritten digits\n",
    "\n",
    "### 3. **Lesson 2: Backpropagation & Transformers**\n",
    "- Understood HOW networks learn\n",
    "- Explored modern architectures\n",
    "- Saw the path to LLMs\n",
    "\n",
    "### 4. **Modern LLMs (Billions of parameters)**\n",
    "- Same principles (layers, activations, backprop)\n",
    "- Scaled massively (data + compute)\n",
    "- Emergent capabilities (reasoning, creativity)\n",
    "\n",
    "---\n",
    "\n",
    "## The Fundamental Principles (Unchanged!):\n",
    "\n",
    "‚úÖ **Layers**: Stack simple transformations  \n",
    "‚úÖ **Activation Functions**: Enable non-linearity  \n",
    "‚úÖ **Loss Functions**: Measure error  \n",
    "‚úÖ **Backpropagation**: Compute gradients  \n",
    "‚úÖ **Gradient Descent**: Update parameters  \n",
    "‚úÖ **Training Data**: Learn patterns  \n",
    "\n",
    "**Everything else is optimization and scale!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Backpropagation**: Chain rule applied recursively to compute gradients\n",
    "2. **Gradient Descent**: Follow gradients downhill to minimize loss\n",
    "3. **Optimizers**: Adam, Momentum improve over standard gradient descent\n",
    "4. **CNNs**: Convolutional layers for spatial data (images)\n",
    "5. **RNNs/LSTMs**: Recurrent connections for sequential data\n",
    "6. **Transformers**: Attention mechanism revolutionized everything\n",
    "7. **LLMs**: Massive Transformers trained on internet-scale data\n",
    "\n",
    "### The Big Picture:\n",
    "\n",
    "**Neural networks are universal function approximators**:\n",
    "- Given enough data and parameters\n",
    "- They can learn almost any input‚Üíoutput mapping\n",
    "- From XOR to language understanding\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- Complete the **assignment** to apply these concepts\n",
    "- Build your own models with PyTorch or TensorFlow\n",
    "- Explore cutting-edge research (diffusion models, multimodal AI)\n",
    "- Consider ethical implications of AI systems\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Final Challenge\n",
    "\n",
    "**Think about this**: GPT-4 has ~1.7 trillion parameters. If each parameter is a 32-bit float (4 bytes):\n",
    "- Total size: 1.7T √ó 4 bytes = 6.8TB just for weights!\n",
    "- Running inference requires massive computational resources\n",
    "- Training cost: Millions of dollars in compute\n",
    "\n",
    "**Questions to ponder**:\n",
    "1. How do we make AI more efficient?\n",
    "2. What are the environmental costs?\n",
    "3. Who gets access to such powerful models?\n",
    "4. What safeguards do we need?\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** üéâ You now understand the foundations of modern AI!\n",
    "\n",
    "From the humble XOR problem to ChatGPT, it's all the same core ideas, just scaled up with brilliant engineering.\n",
    "\n",
    "**You're ready to build the future of AI.** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
