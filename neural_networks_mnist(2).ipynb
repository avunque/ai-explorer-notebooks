{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Lesson 1B: Handwritten Digit Recognition\n",
    "\n",
    "## Building a Real-World Image Classifier\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand how images become neural network inputs\n",
    "- Work with the famous MNIST dataset\n",
    "- Build a 1-hidden-layer network for classification\n",
    "- Train on 60,000 real images\n",
    "- Evaluate accuracy on test data\n",
    "- Visualize what the network learned\n",
    "\n",
    "**Duration:** ~90 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Introduction to MNIST\n",
    "\n",
    "MNIST (Modified National Institute of Standards and Technology) is the \"Hello World\" of computer vision:\n",
    "\n",
    "- **70,000 images** of handwritten digits (0-9)\n",
    "- **60,000 training images** + **10,000 test images**\n",
    "- Each image is **28√ó28 pixels** in grayscale\n",
    "- **784 total pixels** per image (28√ó28)\n",
    "- Pixel values range from **0 (black) to 255 (white)**\n",
    "\n",
    "This dataset has been used to benchmark neural networks since the 1990s!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install and import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load and Explore the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "print(\"üì• Loading MNIST dataset (this may take a moment)...\\n\")\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "X, y = mnist.data.values, mnist.target.values.astype(int)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded!\")\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"  ‚Ä¢ Total images: {X.shape[0]:,}\")\n",
    "print(f\"  ‚Ä¢ Pixels per image: {X.shape[1]} (28√ó28)\")\n",
    "print(f\"  ‚Ä¢ Classes: {len(np.unique(y))} (digits 0-9)\")\n",
    "print(f\"  ‚Ä¢ Data type: {X.dtype}\")\n",
    "print(f\"  ‚Ä¢ Value range: [{X.min():.0f}, {X.max():.0f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "fig.suptitle('Sample MNIST Digits', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Display image\n",
    "    image = X[i].reshape(28, 28)\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.set_title(f'Label: {y[i]}', fontsize=14, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüñºÔ∏è Each 28√ó28 image becomes a 784-dimensional input vector for the neural network!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: From Images to Neural Network Inputs\n",
    "\n",
    "### How do images become inputs?\n",
    "\n",
    "1. **Original:** 28√ó28 pixel grid (2D)\n",
    "2. **Flatten:** Convert to 784-length vector (1D)\n",
    "3. **Normalize:** Scale pixel values from [0, 255] to [0, 1]\n",
    "\n",
    "```\n",
    "Image (28√ó28)          Flatten           Normalize\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>           ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>\n",
    "‚îÇ ‚ñ° ‚ñ° ‚ñ† ‚ñ° ‚îÇ     [0, 0, 255, 0, ...]    [0, 0, 1, 0, ...]\n",
    "‚îÇ ‚ñ° ‚ñ† ‚ñ† ‚ñ° ‚îÇ\n",
    "‚îÇ ‚ñ† ‚ñ° ‚ñ† ‚ñ° ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "   784 pixels ‚Üí 784 inputs to the neural network\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the transformation\n",
    "sample_idx = 0\n",
    "sample_image = X[sample_idx].reshape(28, 28)\n",
    "sample_flat = X[sample_idx]\n",
    "\n",
    "print(f\"üîç Examining digit '{y[sample_idx]}':\\n\")\n",
    "print(f\"Original shape: {sample_image.shape} (28√ó28 grid)\")\n",
    "print(f\"Flattened shape: {sample_flat.shape} (784-element vector)\")\n",
    "print(f\"\\nFirst 20 pixel values (before normalization):\")\n",
    "print(sample_flat[:20])\n",
    "\n",
    "# Visualize one row of pixels\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Show the image\n",
    "axes[0].imshow(sample_image, cmap='gray')\n",
    "axes[0].set_title(f'Original Image: Digit {y[sample_idx]}', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Show pixel intensity profile\n",
    "row_14 = sample_image[14, :]  # Middle row\n",
    "axes[1].plot(row_14, 'b-', linewidth=2, marker='o')\n",
    "axes[1].set_xlabel('Pixel Position (0-27)', fontsize=12)\n",
    "axes[1].set_ylabel('Pixel Intensity (0-255)', fontsize=12)\n",
    "axes[1].set_title('Pixel Values in Row 14', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_normalized = X / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "def to_one_hot(y, num_classes=10):\n",
    "    \"\"\"Convert class labels to one-hot encoded vectors\"\"\"\n",
    "    one_hot = np.zeros((len(y), num_classes))\n",
    "    one_hot[np.arange(len(y)), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_one_hot = to_one_hot(y)\n",
    "\n",
    "print(\"üîß Data Preprocessing:\")\n",
    "print(f\"  ‚Ä¢ Normalized pixel values: {X_normalized.min():.1f} to {X_normalized.max():.1f}\")\n",
    "print(f\"  ‚Ä¢ One-hot encoding shape: {y_one_hot.shape}\")\n",
    "print(f\"\\nExample one-hot encoding for digit {y[0]}:\")\n",
    "print(y_one_hot[0])\n",
    "print(\"\\nüí° One-hot encoding: [0,0,0,1,0,0,0,0,0,0] means the digit is '3' (index 3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "# Use first 60,000 for training, last 10,000 for testing (standard MNIST split)\n",
    "X_train = X_normalized[:60000]\n",
    "y_train = y_one_hot[:60000]\n",
    "y_train_labels = y[:60000]\n",
    "\n",
    "X_test = X_normalized[60000:]\n",
    "y_test = y_one_hot[60000:]\n",
    "y_test_labels = y[60000:]\n",
    "\n",
    "print(\"üìä Dataset Split:\")\n",
    "print(f\"  ‚Ä¢ Training set: {X_train.shape[0]:,} images\")\n",
    "print(f\"  ‚Ä¢ Test set: {X_test.shape[0]:,} images\")\n",
    "print(f\"\\n‚úÖ Data ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Function Explanations Available!\n",
    "\n",
    "**Open this notebook for detailed line-by-line explanations:**\n",
    "\n",
    "üîó **`neural_networks_mnist_function_explanations.ipynb`**\n",
    "\n",
    "Covers:\n",
    "- `compute_loss()` - How to measure prediction errors\n",
    "- `backward()` - How backpropagation works\n",
    "- `train()` - The complete training loop\n",
    "- `predict()` - Making predictions on new data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Network Architecture\n",
    "\n",
    "We'll build a simple 3-layer neural network:\n",
    "\n",
    "```\n",
    "Input Layer     Hidden Layer     Output Layer\n",
    "(784 neurons)   (128 neurons)    (10 neurons)\n",
    "    ‚îÇ                 ‚îÇ                ‚îÇ\n",
    "    ‚îÇ                 ‚îÇ                ‚îÇ\n",
    "  [pixels]    ‚Üí   [features]   ‚Üí   [classes]\n",
    "                ReLU activation   Softmax activation\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "- Input ‚Üí Hidden: 784 √ó 128 = 100,352 weights + 128 biases\n",
    "- Hidden ‚Üí Output: 128 √ó 10 = 1,280 weights + 10 biases\n",
    "- **Total: 101,770 parameters**\n",
    "\n",
    "Much larger than XOR, but still relatively small for neural networks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions\n",
    "def relu(x):\n",
    "    \"\"\"ReLU: Rectified Linear Unit\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"Derivative of ReLU\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax activation for output layer\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability trick\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Visualize ReLU\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y_relu = relu(x)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x, y_relu, 'b-', linewidth=2, label='ReLU(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Input (x)', fontsize=12)\n",
    "plt.ylabel('Output', fontsize=12)\n",
    "plt.title('ReLU Activation Function: max(0, x)', fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='r', linestyle='--', alpha=0.5, label='Activation threshold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Activation functions defined\")\n",
    "print(\"\\nüîç ReLU Properties:\")\n",
    "print(\"  ‚Ä¢ Output range: [0, ‚àû)\")\n",
    "print(\"  ‚Ä¢ ReLU(-2) = 0\")\n",
    "print(\"  ‚Ä¢ ReLU(2) = 2\")\n",
    "print(\"  ‚Ä¢ Faster to compute than sigmoid\")\n",
    "print(\"  ‚Ä¢ Helps avoid vanishing gradients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitRecognitionNetwork:\n",
    "    def __init__(self, input_size=784, hidden_size=128, output_size=10):\n",
    "        \"\"\"Initialize the neural network\"\"\"\n",
    "        # Xavier initialization for better training\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        # Training history\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation\"\"\"\n",
    "        # Input ‚Üí Hidden\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = relu(self.z1)\n",
    "        \n",
    "        # Hidden ‚Üí Output\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = softmax(self.z2)\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Cross-entropy loss\"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        log_likelihood = -np.log(y_pred[range(m), y_true.argmax(axis=1)])\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y_true, learning_rate=0.01):\n",
    "        \"\"\"Backpropagation\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = self.a2 - y_true\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dz1 = np.dot(dz2, self.W2.T) * relu_derivative(self.z1)\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def train(self, X_train, y_train, X_test, y_test, epochs=10, batch_size=128, learning_rate=0.1):\n",
    "        \"\"\"Train the network with mini-batch gradient descent\"\"\"\n",
    "        n_batches = len(X_train) // batch_size\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle training data\n",
    "            indices = np.random.permutation(len(X_train))\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train[indices]\n",
    "            \n",
    "            # Mini-batch training\n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                \n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # Forward and backward pass\n",
    "                self.forward(X_batch)\n",
    "                self.backward(X_batch, y_batch, learning_rate)\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            train_pred = self.forward(X_train)\n",
    "            test_pred = self.forward(X_test)\n",
    "            \n",
    "            train_loss = self.compute_loss(y_train, train_pred)\n",
    "            test_loss = self.compute_loss(y_test, test_pred)\n",
    "            \n",
    "            train_acc = np.mean(np.argmax(train_pred, axis=1) == np.argmax(y_train, axis=1))\n",
    "            test_acc = np.mean(np.argmax(test_pred, axis=1) == np.argmax(y_test, axis=1))\n",
    "            \n",
    "            self.loss_history.append(test_loss)\n",
    "            self.accuracy_history.append(test_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1:2d}/{epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "                  f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Training complete! Final test accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        probabilities = self.forward(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "# Create the network\n",
    "nn = DigitRecognitionNetwork(input_size=784, hidden_size=128, output_size=10)\n",
    "\n",
    "print(\"üß† Neural Network Created\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  Input Layer:  784 neurons (28√ó28 pixels)\")\n",
    "print(f\"  Hidden Layer: 128 neurons (ReLU activation)\")\n",
    "print(f\"  Output Layer: 10 neurons (Softmax activation)\")\n",
    "print(f\"\\nTotal Parameters: {784*128 + 128 + 128*10 + 10:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Train the Network! üöÄ\n",
    "\n",
    "Now we'll train on 60,000 images. This will take a few minutes!\n",
    "\n",
    "**What's happening:**\n",
    "- Network sees each training image\n",
    "- Makes a prediction (which digit?)\n",
    "- Compares with true label\n",
    "- Adjusts weights to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "print(\"üèãÔ∏è Training the neural network on 60,000 handwritten digits...\\n\")\n",
    "nn.train(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss over time\n",
    "axes[0].plot(nn.loss_history, 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Test Loss', fontsize=12)\n",
    "axes[0].set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy over time\n",
    "axes[1].plot([acc*100 for acc in nn.accuracy_history], 'g-', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Test Accuracy Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìà Performance Summary:\")\n",
    "print(f\"  ‚Ä¢ Initial accuracy: {nn.accuracy_history[0]*100:.2f}%\")\n",
    "print(f\"  ‚Ä¢ Final accuracy: {nn.accuracy_history[-1]*100:.2f}%\")\n",
    "print(f\"  ‚Ä¢ Improvement: +{(nn.accuracy_history[-1] - nn.accuracy_history[0])*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Test the Network\n",
    "\n",
    "Let's see how well the network performs on images it has never seen before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "predictions = nn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test_labels, predictions)\n",
    "\n",
    "print(f\"üéØ Test Set Performance:\")\n",
    "print(f\"  ‚Ä¢ Tested on: {len(X_test):,} images\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Correct: {int(accuracy * len(X_test)):,} images\")\n",
    "print(f\"  ‚Ä¢ Incorrect: {len(X_test) - int(accuracy * len(X_test)):,} images\")\n",
    "\n",
    "if accuracy > 0.95:\n",
    "    print(\"\\nüèÜ Excellent! The network achieved >95% accuracy!\")\n",
    "elif accuracy > 0.90:\n",
    "    print(\"\\nüëç Good performance! Above 90% accuracy.\")\n",
    "else:\n",
    "    print(\"\\n‚ö° The network is learning but could use more training epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample predictions\n",
    "fig, axes = plt.subplots(3, 6, figsize=(15, 8))\n",
    "fig.suptitle('Sample Predictions from Test Set', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = np.random.randint(0, len(X_test))\n",
    "    image = X_test[idx].reshape(28, 28)\n",
    "    true_label = y_test_labels[idx]\n",
    "    pred_label = predictions[idx]\n",
    "    \n",
    "    ax.imshow(image, cmap='gray')\n",
    "    \n",
    "    if true_label == pred_label:\n",
    "        ax.set_title(f'‚úÖ True: {true_label}, Pred: {pred_label}', \n",
    "                    fontsize=11, color='green', fontweight='bold')\n",
    "    else:\n",
    "        ax.set_title(f'‚ùå True: {true_label}, Pred: {pred_label}', \n",
    "                    fontsize=11, color='red', fontweight='bold')\n",
    "    \n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Confusion Matrix - Where Does It Fail?\n",
    "\n",
    "A confusion matrix shows which digits the network confuses with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test_labels, predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=range(10), yticklabels=range(10),\n",
    "           cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "plt.title('Confusion Matrix\\n(Diagonal = Correct Predictions)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Confusion Matrix Insights:\")\n",
    "print(\"  ‚Ä¢ Diagonal values = correct predictions\")\n",
    "print(\"  ‚Ä¢ Off-diagonal = mistakes\")\n",
    "print(\"\\nüîç Common confusions:\")\n",
    "\n",
    "# Find most common confusions\n",
    "confusions = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i != j and cm[i, j] > 10:\n",
    "            confusions.append((i, j, cm[i, j]))\n",
    "\n",
    "confusions.sort(key=lambda x: x[2], reverse=True)\n",
    "for true_digit, pred_digit, count in confusions[:5]:\n",
    "    print(f\"  ‚Ä¢ Confused '{true_digit}' as '{pred_digit}': {count} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Visualize What the Network Learned\n",
    "\n",
    "Let's peek inside the hidden layer to see what features it learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hidden layer weights\n",
    "# Each hidden neuron has 784 input weights (one per pixel)\n",
    "fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "fig.suptitle('Hidden Layer Features (First 32 neurons)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Reshape weights to 28√ó28 to visualize as an image\n",
    "    weights = nn.W1[:, i].reshape(28, 28)\n",
    "    ax.imshow(weights, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    ax.set_title(f'Neuron {i}', fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüß† What are we seeing?\")\n",
    "print(\"  ‚Ä¢ Each image shows what ONE hidden neuron 'looks for' in the input\")\n",
    "print(\"  ‚Ä¢ Red areas = positive weights (neuron activates when pixels are bright there)\")\n",
    "print(\"  ‚Ä¢ Blue areas = negative weights (neuron activates when pixels are dark there)\")\n",
    "print(\"  ‚Ä¢ These are the FEATURES the network learned to detect!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Make Your Own Predictions!\n",
    "\n",
    "Test the network on specific examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction function\n",
    "def show_prediction_detail(idx):\n",
    "    \"\"\"Show detailed prediction for a specific test image\"\"\"\n",
    "    image = X_test[idx].reshape(28, 28)\n",
    "    true_label = y_test_labels[idx]\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    probs = nn.forward(X_test[idx:idx+1])[0]\n",
    "    pred_label = np.argmax(probs)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Show image\n",
    "    axes[0].imshow(image, cmap='gray')\n",
    "    axes[0].set_title(f'Test Image #{idx}\\nTrue Label: {true_label}', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Show prediction probabilities\n",
    "    axes[1].bar(range(10), probs * 100, color=['green' if i == pred_label else 'lightblue' for i in range(10)])\n",
    "    axes[1].set_xlabel('Digit', fontsize=12)\n",
    "    axes[1].set_ylabel('Confidence (%)', fontsize=12)\n",
    "    axes[1].set_title(f'Network Prediction: {pred_label} ({probs[pred_label]*100:.1f}% confidence)', \n",
    "                     fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xticks(range(10))\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìä Prediction Breakdown:\")\n",
    "    for digit in range(10):\n",
    "        symbol = \"  ‚Üê PREDICTION\" if digit == pred_label else \"\"\n",
    "        print(f\"  Digit {digit}: {probs[digit]*100:6.2f}%{symbol}\")\n",
    "\n",
    "# Try a few examples\n",
    "print(\"üîç Examining detailed predictions:\\n\")\n",
    "for idx in [0, 100, 500]:\n",
    "    show_prediction_detail(idx)\n",
    "    print(\"\\n\" + \"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Key Takeaways\n",
    "\n",
    "### What We Built:\n",
    "- ‚úÖ A 3-layer neural network (Input ‚Üí Hidden ‚Üí Output)\n",
    "- ‚úÖ Trained on 60,000 handwritten digit images\n",
    "- ‚úÖ Achieved ~95%+ accuracy on unseen test data\n",
    "- ‚úÖ Used 101,770 learnable parameters\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Images ‚Üí Vectors**: 28√ó28 images become 784-element input vectors\n",
    "2. **Hidden Layers Learn Features**: The 128 hidden neurons learned to detect edges, curves, and digit-specific patterns\n",
    "3. **One-Hot Encoding**: 10 output neurons (one per digit) with softmax activation\n",
    "4. **ReLU Activation**: Faster and more effective than sigmoid for hidden layers\n",
    "5. **Mini-Batch Training**: Processing 128 images at a time is more efficient than one-by-one\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Lesson 2**, we'll dive deep into:\n",
    "- The mathematics of backpropagation\n",
    "- How gradient descent really works\n",
    "- Modern architectures: CNNs, Transformers, and LLMs\n",
    "- The principles behind ChatGPT and Claude\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Challenges (Optional)\n",
    "\n",
    "1. **Experiment with architecture**: Try different hidden layer sizes (64, 256, 512). How does it affect accuracy and training time?\n",
    "\n",
    "2. **Add more layers**: Can you create a 2-hidden-layer network? (784 ‚Üí 128 ‚Üí 64 ‚Üí 10)\n",
    "\n",
    "3. **Learning rate tuning**: Test different learning rates (0.01, 0.05, 0.2). What's optimal?\n",
    "\n",
    "4. **Analyze errors**: Find 10 images the network got wrong. Do you see patterns in the mistakes?\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** üéâ You've built and trained a real-world image classification system!\n",
    "\n",
    "**Next:** Move on to **Lesson 2** to understand the theory behind what you just implemented."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
