{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Lesson 1B: Handwritten Digit Recognition (Enhanced)\n",
    "\n",
    "## Building a Real-World Image Classifier - Step by Step with Detailed Explanations\n",
    "\n",
    "**Learning Objectives:**\n",
    "- **Deeply understand** how images become neural network inputs (with visualizations!)\n",
    "- Master the concept of \"flattening\" - turning a 2D image into a 1D vector\n",
    "- Visualize how 784 pixels connect to 784 input neurons\n",
    "- Work with the famous MNIST dataset\n",
    "- Build and train a neural network for classification\n",
    "- Understand every step with concrete examples and graphs\n",
    "\n",
    "**Duration:** ~120 minutes (with time to explore and understand)\n",
    "\n",
    "**Why This Matters:** This lesson prepares you for the text classification assignment. The same principles apply: whether we're processing images or text, we need to convert data into vectors that neural networks can understand!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction to MNIST - The \"Hello World\" of Computer Vision\n",
    "\n",
    "MNIST (Modified National Institute of Standards and Technology) is a dataset of handwritten digits that has been used to train and test machine learning models since the 1990s.\n",
    "\n",
    "### Dataset Facts:\n",
    "- **70,000 images** of handwritten digits (0-9) written by real people\n",
    "- **60,000 training images** (we use these to teach the network)\n",
    "- **10,000 test images** (we use these to evaluate how well it learned)\n",
    "- Each image is **28√ó28 pixels** in grayscale (black and white)\n",
    "- **784 total pixels** per image (28 √ó 28 = 784)\n",
    "- Pixel values range from **0 (black) to 255 (white)**\n",
    "\n",
    "### Why MNIST?\n",
    "- Small enough to train quickly on a laptop\n",
    "- Complex enough to demonstrate real neural network concepts\n",
    "- Universal benchmark - you can compare your results with thousands of other implementations\n",
    "- Perfect stepping stone to understanding how modern computer vision works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install and import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility (so you get the same results every time)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"\\nüìö Ready to learn about image classification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load and Explore the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "print(\"üì• Loading MNIST dataset (this may take a moment - it's downloading 70,000 images!)...\\n\")\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "X, y = mnist.data.values, mnist.target.values.astype(int)\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded!\")\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"  ‚Ä¢ Total images: {X.shape[0]:,}\")\n",
    "print(f\"  ‚Ä¢ Pixels per image: {X.shape[1]} (this is 28√ó28 = 784)\")\n",
    "print(f\"  ‚Ä¢ Classes: {len(np.unique(y))} (digits 0-9)\")\n",
    "print(f\"  ‚Ä¢ Data type: {X.dtype}\")\n",
    "print(f\"  ‚Ä¢ Value range: [{X.min():.0f}, {X.max():.0f}]\")\n",
    "print(f\"\\nüîç Shape of X: {X.shape}\")\n",
    "print(f\"   Translation: {X.shape[0]} images, each with {X.shape[1]} pixel values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images to understand what we're working with\n",
    "fig, axes = plt.subplots(2, 5, figsize=(14, 6))\n",
    "fig.suptitle('Sample MNIST Handwritten Digits', fontsize=18, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Reshape the flat 784-element array back into 28√ó28 for display\n",
    "    image = X[i].reshape(28, 28)\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.set_title(f'Label: {y[i]}', fontsize=14, fontweight='bold', color='blue')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüëÄ What you're seeing:\")\n",
    "print(\"  ‚Ä¢ Real handwritten digits from different people\")\n",
    "print(\"  ‚Ä¢ Different writing styles and shapes\")\n",
    "print(\"  ‚Ä¢ Some are clear, some are messy (just like real handwriting!)\")\n",
    "print(\"  ‚Ä¢ Our neural network will learn to recognize ALL of these variations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: UNDERSTANDING IMAGE FLATTENING - The Most Important Concept!\n",
    "\n",
    "### üéØ This is where many students get confused. Let's break it down step by step!\n",
    "\n",
    "**The Challenge:** Neural networks need 1D vectors as input, but images are 2D grids!\n",
    "\n",
    "**The Solution:** We \"flatten\" the 2D grid into a 1D vector.\n",
    "\n",
    "### What Does \"Flattening\" Mean?\n",
    "\n",
    "Imagine you have a chessboard (8√ó8 = 64 squares). Flattening means:\n",
    "1. Take the first row and write it down: squares 1-8\n",
    "2. Take the second row and write it after: squares 9-16\n",
    "3. Continue for all rows...\n",
    "4. Now you have a LINE of 64 squares instead of a grid!\n",
    "\n",
    "For MNIST:\n",
    "- Original: 28 rows √ó 28 columns = **2D grid**\n",
    "- Flattened: 784 elements in a **1D line**\n",
    "- **Same information, different shape!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's demonstrate flattening with a SIMPLE example first\n",
    "# We'll use a tiny 5√ó5 image to make it easy to understand\n",
    "\n",
    "print(\"üìê DEMONSTRATION: Flattening a Simple 5√ó5 Grid\\n\")\n",
    "\n",
    "# Create a simple 5√ó5 pattern\n",
    "simple_image = np.array([\n",
    "    [0, 0, 255, 0, 0],\n",
    "    [0, 255, 255, 255, 0],\n",
    "    [255, 255, 255, 255, 255],\n",
    "    [0, 255, 255, 255, 0],\n",
    "    [0, 0, 255, 0, 0]\n",
    "])\n",
    "\n",
    "print(\"ORIGINAL 2D IMAGE (5√ó5):\")\n",
    "print(simple_image)\n",
    "print(f\"Shape: {simple_image.shape}\\n\")\n",
    "\n",
    "# Flatten it\n",
    "flattened = simple_image.flatten()\n",
    "\n",
    "print(\"FLATTENED 1D VECTOR (25 elements):\")\n",
    "print(flattened)\n",
    "print(f\"Shape: {flattened.shape}\\n\")\n",
    "\n",
    "# Visualize the process\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Show 2D image\n",
    "axes[0].imshow(simple_image, cmap='gray', vmin=0, vmax=255)\n",
    "axes[0].set_title('BEFORE: 5√ó5 Grid (2D)\\n25 values arranged in rows and columns', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, color='red', linewidth=2)\n",
    "axes[0].set_xticks(range(5))\n",
    "axes[0].set_yticks(range(5))\n",
    "\n",
    "# Show 1D flattened version\n",
    "axes[1].imshow(flattened.reshape(1, -1), cmap='gray', vmin=0, vmax=255, aspect='auto')\n",
    "axes[1].set_title('AFTER: 25-element Vector (1D)\\nSame 25 values in a single line', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Position (0-24)', fontsize=11)\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç KEY INSIGHT:\")\n",
    "print(\"  ‚úì Same pixel values (same brightness)\")\n",
    "print(\"  ‚úì Same information (same pattern)\")\n",
    "print(\"  ‚úì Different shape (2D ‚Üí 1D)\")\n",
    "print(\"  ‚úì Now compatible with neural network input!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Flattening a REAL MNIST Image (28√ó28)\n",
    "\n",
    "Now let's apply the same concept to a real MNIST digit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a real MNIST digit and demonstrate flattening\n",
    "sample_idx = 0\n",
    "sample_image_2d = X[sample_idx].reshape(28, 28)  # 2D version (28√ó28)\n",
    "sample_image_1d = X[sample_idx]                   # 1D version (784)\n",
    "\n",
    "print(f\"üîç Examining MNIST digit '{y[sample_idx]}':\\n\")\n",
    "print(f\"ORIGINAL IMAGE:\")\n",
    "print(f\"  ‚Ä¢ Shape: {sample_image_2d.shape} (28 rows, 28 columns)\")\n",
    "print(f\"  ‚Ä¢ Total pixels: 28 √ó 28 = {28*28}\")\n",
    "print(f\"  ‚Ä¢ This is a 2D GRID\\n\")\n",
    "\n",
    "print(f\"FLATTENED IMAGE:\")\n",
    "print(f\"  ‚Ä¢ Shape: {sample_image_1d.shape} (784 elements in a line)\")\n",
    "print(f\"  ‚Ä¢ This is a 1D VECTOR\")\n",
    "print(f\"  ‚Ä¢ First 20 pixel values: {sample_image_1d[:20]}\")\n",
    "print(f\"\\nüí° The flattened version has the SAME pixel values, just rearranged into a line!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize HOW flattening works row-by-row\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Top: Show the 2D image\n",
    "ax1 = plt.subplot(3, 1, 1)\n",
    "im = ax1.imshow(sample_image_2d, cmap='gray')\n",
    "ax1.set_title(f'STEP 1: Original 28√ó28 Image (Digit: {y[sample_idx]})', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "ax1.set_xlabel('Column (0-27)', fontsize=12)\n",
    "ax1.set_ylabel('Row (0-27)', fontsize=12)\n",
    "plt.colorbar(im, ax=ax1, label='Pixel Intensity (0=black, 255=white)')\n",
    "\n",
    "# Middle: Show how rows stack\n",
    "ax2 = plt.subplot(3, 1, 2)\n",
    "# Show a few rows highlighted\n",
    "ax2.imshow(sample_image_2d, cmap='gray', alpha=0.3)\n",
    "ax2.axhline(y=0.5, color='red', linewidth=3, label='Row 0 (first 28 pixels)')\n",
    "ax2.axhline(y=5.5, color='orange', linewidth=3, label='Row 5')\n",
    "ax2.axhline(y=13.5, color='yellow', linewidth=3, label='Row 13 (middle)')\n",
    "ax2.set_title('STEP 2: Each ROW becomes a segment of the flattened vector', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "ax2.legend(loc='upper right', fontsize=10)\n",
    "ax2.set_xlabel('28 pixels per row', fontsize=12)\n",
    "ax2.set_ylabel('28 rows total', fontsize=12)\n",
    "\n",
    "# Bottom: Show the flattened vector with row markers\n",
    "ax3 = plt.subplot(3, 1, 3)\n",
    "ax3.imshow(sample_image_1d.reshape(1, -1), cmap='gray', aspect='auto')\n",
    "ax3.set_title('STEP 3: Flattened into 784-element vector (28 rows √ó 28 pixels = 784)', \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "ax3.set_xlabel('Position in vector (0-783)', fontsize=12)\n",
    "ax3.set_yticks([])\n",
    "\n",
    "# Add markers showing where each row starts\n",
    "for row in [0, 5, 13, 27]:\n",
    "    pos = row * 28\n",
    "    ax3.axvline(x=pos, color='red', linewidth=2, alpha=0.5)\n",
    "    ax3.text(pos, 0.5, f'Row {row}', rotation=90, fontsize=9, \n",
    "             verticalalignment='center', color='red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä HOW FLATTENING WORKS:\")\n",
    "print(\"  Row 0 (pixels 0-27)   ‚Üí Vector positions 0-27\")\n",
    "print(\"  Row 1 (pixels 0-27)   ‚Üí Vector positions 28-55\")\n",
    "print(\"  Row 2 (pixels 0-27)   ‚Üí Vector positions 56-83\")\n",
    "print(\"  ...\")\n",
    "print(\"  Row 27 (pixels 0-27)  ‚Üí Vector positions 756-783\")\n",
    "print(\"\\nüéØ RESULT: One long line with all 784 pixel values!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show pixel intensity profile to make the data more concrete\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Top-left: The image\n",
    "axes[0, 0].imshow(sample_image_2d, cmap='gray')\n",
    "axes[0, 0].set_title(f'Digit: {y[sample_idx]}', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axhline(y=14, color='red', linewidth=2, linestyle='--', label='Row 14')\n",
    "axes[0, 0].axvline(x=14, color='blue', linewidth=2, linestyle='--', label='Column 14')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_xlabel('Column', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Row', fontsize=11)\n",
    "\n",
    "# Top-right: One row's pixel values\n",
    "row_14 = sample_image_2d[14, :]\n",
    "axes[0, 1].plot(row_14, 'r-', linewidth=2, marker='o', markersize=4)\n",
    "axes[0, 1].set_xlabel('Column Position (0-27)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Pixel Intensity', fontsize=11)\n",
    "axes[0, 1].set_title('Pixel Values in Row 14 (28 values)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_ylim(-10, 265)\n",
    "\n",
    "# Bottom-left: One column's pixel values\n",
    "col_14 = sample_image_2d[:, 14]\n",
    "axes[1, 0].plot(col_14, 'b-', linewidth=2, marker='o', markersize=4)\n",
    "axes[1, 0].set_xlabel('Row Position (0-27)', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Pixel Intensity', fontsize=11)\n",
    "axes[1, 0].set_title('Pixel Values in Column 14 (28 values)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].set_ylim(-10, 265)\n",
    "\n",
    "# Bottom-right: First 100 pixels of flattened vector\n",
    "axes[1, 1].plot(sample_image_1d[:100], 'g-', linewidth=1.5)\n",
    "axes[1, 1].set_xlabel('Position in Flattened Vector', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Pixel Intensity', fontsize=11)\n",
    "axes[1, 1].set_title('First 100 Values of Flattened Vector', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_ylim(-10, 265)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüî¨ UNDERSTANDING THE DATA:\")\n",
    "print(f\"  ‚Ä¢ Row 14 has {len(row_14)} pixel values\")\n",
    "print(f\"  ‚Ä¢ Column 14 has {len(col_14)} pixel values\")\n",
    "print(f\"  ‚Ä¢ Flattened vector has {len(sample_image_1d)} total values (28 rows √ó 28 columns)\")\n",
    "print(f\"  ‚Ä¢ Each spike in the graphs represents a bright pixel (part of the digit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: FROM FLATTENED VECTOR TO NEURAL NETWORK INPUT\n",
    "\n",
    "### üß† This is the connection students often miss!\n",
    "\n",
    "**Question:** We have a 784-element vector. Now what?\n",
    "\n",
    "**Answer:** Each element of the vector connects to ONE neuron in the input layer!\n",
    "\n",
    "### The Architecture:\n",
    "```\n",
    "Image (28√ó28)  ‚Üí  Flatten  ‚Üí  784-element vector  ‚Üí  784 Input Neurons\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÄ‚îÄ‚îÄ‚îÄ>       [p‚ÇÄ, p‚ÇÅ, ..., p‚Çá‚Çà‚ÇÉ]        ‚óè  ‚óè  ‚óè\n",
    "   ‚îÇ ‚ñ† ‚ñ° ‚îÇ                                               ‚óè  ‚óè  ‚óè\n",
    "   ‚îÇ ‚ñ° ‚ñ† ‚îÇ                                              ‚óè  ‚óè  ‚óè\n",
    "   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                            (784 neurons)\n",
    "```\n",
    "\n",
    "**Each pixel value goes to exactly ONE input neuron!**\n",
    "- Pixel 0 ‚Üí Neuron 0\n",
    "- Pixel 1 ‚Üí Neuron 1\n",
    "- ...\n",
    "- Pixel 783 ‚Üí Neuron 783"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the neural network architecture\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Left: The image\n",
    "axes[0].imshow(sample_image_2d, cmap='gray')\n",
    "axes[0].set_title('1. Original Image\\n(28√ó28 = 784 pixels)', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Middle: The flattened vector\n",
    "axes[1].imshow(sample_image_1d.reshape(1, -1), cmap='gray', aspect='auto')\n",
    "axes[1].set_title('2. Flattened Vector\\n(784 elements in a line)', \n",
    "                  fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Index: 0 ‚Üí 783', fontsize=11)\n",
    "axes[1].set_yticks([])\n",
    "\n",
    "# Right: Representation of input layer\n",
    "# We'll show a subset of neurons to make it visible\n",
    "axes[2].text(0.5, 0.9, 'Input Layer\\n(784 neurons)', \n",
    "            ha='center', va='top', fontsize=14, fontweight='bold',\n",
    "            transform=axes[2].transAxes)\n",
    "axes[2].text(0.5, 0.75, '‚óè', ha='center', va='center', fontsize=40, \n",
    "            transform=axes[2].transAxes, color='blue')\n",
    "axes[2].text(0.3, 0.65, '‚óè', ha='center', va='center', fontsize=40, \n",
    "            transform=axes[2].transAxes, color='blue')\n",
    "axes[2].text(0.7, 0.65, '‚óè', ha='center', va='center', fontsize=40, \n",
    "            transform=axes[2].transAxes, color='blue')\n",
    "axes[2].text(0.2, 0.55, '‚óè', ha='center', va='center', fontsize=40, \n",
    "            transform=axes[2].transAxes, color='blue')\n",
    "axes[2].text(0.5, 0.55, '‚óè', ha='center', va='center', fontsize=40, \n",
    "            transform=axes[2].transAxes, color='blue')\n",
    "axes[2].text(0.8, 0.55, '‚óè', ha='center', va='center', fontsize=40, \n",
    "            transform=axes[2].transAxes, color='blue')\n",
    "axes[2].text(0.5, 0.4, '‚ãÆ\\n(784 total)', ha='center', va='center', \n",
    "            fontsize=20, transform=axes[2].transAxes)\n",
    "axes[2].text(0.3, 0.25, '‚óè', ha='center', va='center', fontsize=40, \n",
    "            transform=axes[2].transAxes, color='blue')\n",
    "axes[2].text(0.7, 0.25, '‚óè', ha='center', va='center', fontsize=40, \n",
    "            transform=axes[2].transAxes, color='blue')\n",
    "\n",
    "axes[2].text(0.5, 0.05, 'Each neuron receives\\nONE pixel value', \n",
    "            ha='center', va='bottom', fontsize=11, style='italic',\n",
    "            transform=axes[2].transAxes)\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ THE CONNECTION:\")\n",
    "print(\"  1. Start with 28√ó28 image (2D grid)\")\n",
    "print(\"  2. Flatten to 784-element vector (1D array)\")\n",
    "print(\"  3. Feed each element to ONE input neuron (784 neurons)\")\n",
    "print(\"\\nüí° WHY 784 INPUT NEURONS?\")\n",
    "print(\"  Because we have 784 pixel values to process!\")\n",
    "print(\"  Each neuron is responsible for ONE pixel location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Data Preprocessing - Preparing for the Neural Network\n",
    "\n",
    "Before training, we need to:\n",
    "1. **Normalize** pixel values from [0, 255] to [0, 1]\n",
    "2. **One-hot encode** labels (e.g., digit \"3\" ‚Üí [0,0,0,1,0,0,0,0,0,0])\n",
    "3. **Split** into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Normalize pixel values\n",
    "print(\"üîß STEP 1: Normalizing pixel values\\n\")\n",
    "print(f\"Before normalization: min={X.min()}, max={X.max()}\")\n",
    "\n",
    "X_normalized = X / 255.0\n",
    "\n",
    "print(f\"After normalization:  min={X_normalized.min()}, max={X_normalized.max()}\")\n",
    "print(\"\\nüí° Why normalize?\")\n",
    "print(\"  ‚Ä¢ Neural networks work better with small numbers (0-1 instead of 0-255)\")\n",
    "print(\"  ‚Ä¢ Helps with gradient descent and prevents numerical instability\")\n",
    "print(\"  ‚Ä¢ Standard practice in machine learning!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: One-hot encoding\n",
    "def to_one_hot(y, num_classes=10):\n",
    "    \"\"\"Convert class labels to one-hot encoded vectors\"\"\"\n",
    "    one_hot = np.zeros((len(y), num_classes))\n",
    "    one_hot[np.arange(len(y)), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_one_hot = to_one_hot(y)\n",
    "\n",
    "print(\"üîß STEP 2: One-hot encoding labels\\n\")\n",
    "print(\"Examples of one-hot encoding:\")\n",
    "for i in range(5):\n",
    "    print(f\"  Digit {y[i]} ‚Üí {y_one_hot[i]}\")\n",
    "\n",
    "print(\"\\nüí° Why one-hot encoding?\")\n",
    "print(\"  ‚Ä¢ Neural network outputs 10 probabilities (one per digit)\")\n",
    "print(\"  ‚Ä¢ We need labels in the same format for comparison\")\n",
    "print(\"  ‚Ä¢ [0,0,0,1,0,0,0,0,0,0] means 'this is definitely a 3'\")\n",
    "print(\"  ‚Ä¢ Network learns to output similar patterns!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split into training and test sets\n",
    "# Standard MNIST split: first 60,000 for training, last 10,000 for testing\n",
    "X_train = X_normalized[:60000]\n",
    "y_train = y_one_hot[:60000]\n",
    "y_train_labels = y[:60000]\n",
    "\n",
    "X_test = X_normalized[60000:]\n",
    "y_test = y_one_hot[60000:]\n",
    "y_test_labels = y[60000:]\n",
    "\n",
    "print(\"üîß STEP 3: Splitting dataset\\n\")\n",
    "print(f\"Training set:\")\n",
    "print(f\"  ‚Ä¢ Images: {X_train.shape[0]:,}\")\n",
    "print(f\"  ‚Ä¢ Each image: {X_train.shape[1]} pixels\")\n",
    "print(f\"  ‚Ä¢ Labels: {y_train.shape[0]:,} one-hot vectors (each is 10 elements)\\n\")\n",
    "\n",
    "print(f\"Test set:\")\n",
    "print(f\"  ‚Ä¢ Images: {X_test.shape[0]:,}\")\n",
    "print(f\"  ‚Ä¢ Each image: {X_test.shape[1]} pixels\")\n",
    "print(f\"  ‚Ä¢ Labels: {y_test.shape[0]:,} one-hot vectors\\n\")\n",
    "\n",
    "print(\"‚úÖ Data ready for training!\")\n",
    "print(\"\\nüí° Why separate train/test?\")\n",
    "print(\"  ‚Ä¢ Train on 60,000 images to learn patterns\")\n",
    "print(\"  ‚Ä¢ Test on 10,000 UNSEEN images to measure real performance\")\n",
    "print(\"  ‚Ä¢ Prevents overfitting - ensures the network generalizes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Network Architecture Explained in Detail\n",
    "\n",
    "We'll build a 3-layer neural network:\n",
    "\n",
    "```\n",
    "Input Layer          Hidden Layer        Output Layer\n",
    "784 neurons    ‚Üí     128 neurons    ‚Üí    10 neurons\n",
    "[each pixel]         [features]          [digit 0-9]\n",
    "                     ReLU                Softmax\n",
    "```\n",
    "\n",
    "### Why this architecture?\n",
    "\n",
    "**Input Layer (784 neurons):**\n",
    "- One neuron per pixel (28√ó28 = 784)\n",
    "- Each receives one pixel's brightness value\n",
    "\n",
    "**Hidden Layer (128 neurons):**\n",
    "- Learns to detect features (edges, curves, loops)\n",
    "- 128 is a good balance: enough to learn patterns, not too many to overtrain\n",
    "- Uses ReLU activation: ReLU(x) = max(0, x)\n",
    "\n",
    "**Output Layer (10 neurons):**\n",
    "- One neuron per digit (0, 1, 2, ..., 9)\n",
    "- Uses Softmax: converts scores to probabilities that sum to 1\n",
    "- The neuron with highest probability is the prediction!\n",
    "\n",
    "### How many parameters?\n",
    "- Input ‚Üí Hidden: (784 √ó 128) + 128 bias = **100,480**\n",
    "- Hidden ‚Üí Output: (128 √ó 10) + 10 bias = **1,290**\n",
    "- **Total: 101,770 parameters** that the network will learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions with detailed explanations\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU: Rectified Linear Unit - outputs x if x>0, else 0\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"Derivative of ReLU: 1 if x>0, else 0\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Softmax: converts scores to probabilities that sum to 1\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability trick\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Visualize activation functions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# ReLU visualization\n",
    "x = np.linspace(-3, 3, 200)\n",
    "y_relu = relu(x)\n",
    "\n",
    "axes[0].plot(x, y_relu, 'b-', linewidth=3, label='ReLU(x) = max(0, x)')\n",
    "axes[0].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "axes[0].axvline(x=0, color='r', linestyle='--', alpha=0.5, linewidth=2)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xlabel('Input (x)', fontsize=13)\n",
    "axes[0].set_ylabel('Output', fontsize=13)\n",
    "axes[0].set_title('ReLU Activation Function', fontsize=15, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].text(-2.5, 2, 'Negative inputs ‚Üí 0\\nPositive inputs ‚Üí pass through', \n",
    "            fontsize=11, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Softmax visualization example\n",
    "raw_scores = np.array([2.0, 1.0, 0.1, -1.0, -2.0])\n",
    "probabilities = softmax(raw_scores.reshape(1, -1))[0]\n",
    "\n",
    "x_pos = np.arange(len(raw_scores))\n",
    "axes[1].bar(x_pos, probabilities, color=['red', 'orange', 'yellow', 'lightblue', 'lightblue'],\n",
    "           edgecolor='black', linewidth=1.5)\n",
    "axes[1].set_xlabel('Output Neuron', fontsize=13)\n",
    "axes[1].set_ylabel('Probability', fontsize=13)\n",
    "axes[1].set_title('Softmax Example: Scores ‚Üí Probabilities', fontsize=15, fontweight='bold')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels([f'#{i}' for i in range(5)])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, (score, prob) in enumerate(zip(raw_scores, probabilities)):\n",
    "    axes[1].text(i, prob + 0.02, f'{prob:.3f}', ha='center', fontsize=10, fontweight='bold')\n",
    "    axes[1].text(i, -0.05, f'score: {score}', ha='center', fontsize=9, style='italic')\n",
    "\n",
    "axes[1].text(2, 0.4, f'Sum = {probabilities.sum():.1f}\\n(always 1.0!)', \n",
    "            ha='center', fontsize=11, bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Activation functions explained:\")\n",
    "print(\"\\nüîµ ReLU (for hidden layer):\")\n",
    "print(\"  ‚Ä¢ Simple: if input > 0, pass it through; else output 0\")\n",
    "print(\"  ‚Ä¢ Fast to compute\")\n",
    "print(\"  ‚Ä¢ Helps network learn non-linear patterns\")\n",
    "print(\"  ‚Ä¢ Example: ReLU(-2) = 0, ReLU(3) = 3\")\n",
    "print(\"\\nüî¥ Softmax (for output layer):\")\n",
    "print(\"  ‚Ä¢ Converts raw scores to probabilities\")\n",
    "print(\"  ‚Ä¢ All outputs sum to 1.0\")\n",
    "print(\"  ‚Ä¢ Largest score gets largest probability\")\n",
    "print(\"  ‚Ä¢ Network can express confidence: 'I'm 95% sure this is a 3'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Building the Neural Network Class\n",
    "\n",
    "Now we'll implement the complete neural network with detailed comments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitRecognitionNetwork:\n",
    "    def __init__(self, input_size=784, hidden_size=128, output_size=10):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with random weights\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size: 784 (one per pixel)\n",
    "        - hidden_size: 128 (number of hidden neurons)\n",
    "        - output_size: 10 (one per digit 0-9)\n",
    "        \"\"\"\n",
    "        # Xavier initialization: smart random values that help training\n",
    "        # Weights between input and hidden layer\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))  # Biases for hidden layer\n",
    "        \n",
    "        # Weights between hidden and output layer\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))  # Biases for output layer\n",
    "        \n",
    "        # Track training progress\n",
    "        self.loss_history = []\n",
    "        self.accuracy_history = []\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation: compute predictions\n",
    "        \n",
    "        X shape: (batch_size, 784) - batch of flattened images\n",
    "        Returns: (batch_size, 10) - probabilities for each digit\n",
    "        \"\"\"\n",
    "        # Layer 1: Input ‚Üí Hidden\n",
    "        # z1 = X ¬∑ W1 + b1  (linear transformation)\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1  # Shape: (batch_size, 128)\n",
    "        self.a1 = relu(self.z1)                  # Apply ReLU activation\n",
    "        \n",
    "        # Layer 2: Hidden ‚Üí Output\n",
    "        # z2 = a1 ¬∑ W2 + b2  (linear transformation)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2  # Shape: (batch_size, 10)\n",
    "        self.a2 = softmax(self.z2)                     # Apply Softmax\n",
    "        \n",
    "        return self.a2  # Probabilities for each digit\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute cross-entropy loss\n",
    "        \n",
    "        Lower loss = better predictions\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]  # Number of examples\n",
    "        # Cross-entropy: -log(probability of correct class)\n",
    "        log_likelihood = -np.log(y_pred[range(m), y_true.argmax(axis=1)] + 1e-8)\n",
    "        loss = np.sum(log_likelihood) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y_true, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Backpropagation: compute gradients and update weights\n",
    "        \n",
    "        This is where the learning happens!\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Step 1: Compute gradients for output layer\n",
    "        dz2 = self.a2 - y_true  # Error at output\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m  # Gradient for W2\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m  # Gradient for b2\n",
    "        \n",
    "        # Step 2: Compute gradients for hidden layer\n",
    "        dz1 = np.dot(dz2, self.W2.T) * relu_derivative(self.z1)  # Error at hidden\n",
    "        dW1 = np.dot(X.T, dz1) / m  # Gradient for W1\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m  # Gradient for b1\n",
    "        \n",
    "        # Step 3: Update weights (gradient descent)\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def train(self, X_train, y_train, X_test, y_test, epochs=10, batch_size=128, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        Train the network using mini-batch gradient descent\n",
    "        \n",
    "        Mini-batch: instead of using all 60,000 images at once,\n",
    "        we use small batches of 128 images for efficiency\n",
    "        \"\"\"\n",
    "        n_batches = len(X_train) // batch_size\n",
    "        \n",
    "        print(f\"Training for {epochs} epochs with batch size {batch_size}\")\n",
    "        print(f\"Number of batches per epoch: {n_batches}\\n\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle training data each epoch (prevents memorization)\n",
    "            indices = np.random.permutation(len(X_train))\n",
    "            X_shuffled = X_train[indices]\n",
    "            y_shuffled = y_train[indices]\n",
    "            \n",
    "            # Train on mini-batches\n",
    "            for i in range(n_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "                \n",
    "                X_batch = X_shuffled[start_idx:end_idx]\n",
    "                y_batch = y_shuffled[start_idx:end_idx]\n",
    "                \n",
    "                # Forward pass: make predictions\n",
    "                self.forward(X_batch)\n",
    "                # Backward pass: learn from mistakes\n",
    "                self.backward(X_batch, y_batch, learning_rate)\n",
    "            \n",
    "            # Evaluate performance after each epoch\n",
    "            train_pred = self.forward(X_train)\n",
    "            test_pred = self.forward(X_test)\n",
    "            \n",
    "            train_loss = self.compute_loss(y_train, train_pred)\n",
    "            test_loss = self.compute_loss(y_test, test_pred)\n",
    "            \n",
    "            train_acc = np.mean(np.argmax(train_pred, axis=1) == np.argmax(y_train, axis=1))\n",
    "            test_acc = np.mean(np.argmax(test_pred, axis=1) == np.argmax(y_test, axis=1))\n",
    "            \n",
    "            self.loss_history.append(test_loss)\n",
    "            self.accuracy_history.append(test_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1:2d}/{epochs} | \"\n",
    "                  f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "                  f\"Test Loss: {test_loss:.4f} Acc: {test_acc:.4f}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Training complete!\")\n",
    "        print(f\"Final test accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions: return the digit with highest probability\"\"\"\n",
    "        probabilities = self.forward(X)\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "# Create the network\n",
    "print(\"üß† Creating Neural Network...\\n\")\n",
    "nn = DigitRecognitionNetwork(input_size=784, hidden_size=128, output_size=10)\n",
    "\n",
    "print(\"Architecture Summary:\")\n",
    "print(f\"  Input Layer:    784 neurons (one per pixel)\")\n",
    "print(f\"  Hidden Layer:   128 neurons (ReLU activation)\")\n",
    "print(f\"  Output Layer:    10 neurons (Softmax activation)\")\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  W1 shape: {nn.W1.shape} ‚Üí {nn.W1.size:,} weights\")\n",
    "print(f\"  b1 shape: {nn.b1.shape} ‚Üí {nn.b1.size:,} biases\")\n",
    "print(f\"  W2 shape: {nn.W2.shape} ‚Üí {nn.W2.size:,} weights\")\n",
    "print(f\"  b2 shape: {nn.b2.shape} ‚Üí {nn.b2.size:,} biases\")\n",
    "print(f\"  Total parameters: {nn.W1.size + nn.b1.size + nn.W2.size + nn.b2.size:,}\")\n",
    "print(\"\\n‚úÖ Network ready to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Train the Network! üöÄ\n",
    "\n",
    "Now for the exciting part - watching the network learn!\n",
    "\n",
    "**What to watch for:**\n",
    "- Loss should decrease (network getting better at predicting)\n",
    "- Accuracy should increase (more correct predictions)\n",
    "- Should reach >95% accuracy after 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "print(\"üèãÔ∏è Training on 60,000 handwritten digits...\\n\")\n",
    "print(\"This will take 2-3 minutes. Watch the accuracy improve!\\n\")\n",
    "\n",
    "nn.train(\n",
    "    X_train, y_train,\n",
    "    X_test, y_test,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Loss over time\n",
    "epochs_range = range(1, len(nn.loss_history) + 1)\n",
    "axes[0].plot(epochs_range, nn.loss_history, 'b-o', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Epoch', fontsize=13)\n",
    "axes[0].set_ylabel('Loss (Cross-Entropy)', fontsize=13)\n",
    "axes[0].set_title('Training Loss Over Time\\n(Lower is Better)', fontsize=15, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks(epochs_range)\n",
    "\n",
    "# Accuracy over time\n",
    "accuracy_pct = [acc * 100 for acc in nn.accuracy_history]\n",
    "axes[1].plot(epochs_range, accuracy_pct, 'g-o', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Epoch', fontsize=13)\n",
    "axes[1].set_ylabel('Test Accuracy (%)', fontsize=13)\n",
    "axes[1].set_title('Test Accuracy Over Time\\n(Higher is Better)', fontsize=15, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 100])\n",
    "axes[1].set_xticks(epochs_range)\n",
    "axes[1].axhline(y=95, color='r', linestyle='--', alpha=0.5, label='95% target')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Performance Summary:\")\n",
    "print(f\"  ‚Ä¢ Starting accuracy: {nn.accuracy_history[0]*100:.2f}%\")\n",
    "print(f\"  ‚Ä¢ Final accuracy: {nn.accuracy_history[-1]*100:.2f}%\")\n",
    "print(f\"  ‚Ä¢ Improvement: +{(nn.accuracy_history[-1] - nn.accuracy_history[0])*100:.2f} percentage points\")\n",
    "print(f\"  ‚Ä¢ Starting loss: {nn.loss_history[0]:.4f}\")\n",
    "print(f\"  ‚Ä¢ Final loss: {nn.loss_history[-1]:.4f}\")\n",
    "\n",
    "if nn.accuracy_history[-1] > 0.95:\n",
    "    print(\"\\nüèÜ Excellent! Achieved >95% accuracy!\")\n",
    "elif nn.accuracy_history[-1] > 0.90:\n",
    "    print(\"\\nüëç Good performance! Above 90%.\")\n",
    "else:\n",
    "    print(\"\\nüí™ Network is learning. Try more epochs for better results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Test the Trained Network\n",
    "\n",
    "Let's see how well it performs on images it has NEVER seen before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "print(\"üéØ Evaluating on Test Set (10,000 unseen images)...\\n\")\n",
    "\n",
    "predictions = nn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test_labels, predictions)\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"  ‚Ä¢ Total test images: {len(X_test):,}\")\n",
    "print(f\"  ‚Ä¢ Correct predictions: {int(accuracy * len(X_test)):,}\")\n",
    "print(f\"  ‚Ä¢ Incorrect predictions: {len(X_test) - int(accuracy * len(X_test)):,}\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "if accuracy > 0.95:\n",
    "    print(\"\\nüèÜ Outstanding! Professional-grade performance!\")\n",
    "    print(\"    This is better than many commercial systems from the 1990s!\")\n",
    "elif accuracy > 0.90:\n",
    "    print(\"\\nüëç Strong performance! The network learned well.\")\n",
    "else:\n",
    "    print(\"\\n‚ö° Room for improvement. Consider more training epochs or tuning hyperparameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample predictions with probabilities\n",
    "fig, axes = plt.subplots(4, 5, figsize=(16, 13))\n",
    "fig.suptitle('Sample Predictions from Test Set', fontsize=18, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    idx = np.random.randint(0, len(X_test))\n",
    "    image = X_test[idx].reshape(28, 28)\n",
    "    true_label = y_test_labels[idx]\n",
    "    pred_label = predictions[idx]\n",
    "    \n",
    "    # Get probabilities\n",
    "    probs = nn.forward(X_test[idx:idx+1])[0]\n",
    "    confidence = probs[pred_label]\n",
    "    \n",
    "    ax.imshow(image, cmap='gray')\n",
    "    \n",
    "    if true_label == pred_label:\n",
    "        ax.set_title(f'‚úÖ Predicted: {pred_label} (confidence: {confidence:.2%})\\nTrue: {true_label}', \n",
    "                    fontsize=10, color='green', fontweight='bold')\n",
    "    else:\n",
    "        ax.set_title(f'‚ùå Predicted: {pred_label} (confidence: {confidence:.2%})\\nTrue: {true_label}', \n",
    "                    fontsize=10, color='red', fontweight='bold')\n",
    "    \n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Understanding the predictions:\")\n",
    "print(\"  ‚Ä¢ Green = correct prediction\")\n",
    "print(\"  ‚Ä¢ Red = incorrect prediction (rare!)\")\n",
    "print(\"  ‚Ä¢ Confidence = how sure the network is (0-100%)\")\n",
    "print(\"  ‚Ä¢ High confidence + correct = network learned well!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Confusion Matrix - Understanding Mistakes\n",
    "\n",
    "A confusion matrix shows which digits the network confuses with each other.\n",
    "\n",
    "**How to read it:**\n",
    "- Rows = true labels (what the digit actually is)\n",
    "- Columns = predicted labels (what the network guessed)\n",
    "- Diagonal = correct predictions (should be largest numbers)\n",
    "- Off-diagonal = mistakes (smaller numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and visualize confusion matrix\n",
    "cm = confusion_matrix(y_test_labels, predictions)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=range(10), yticklabels=range(10),\n",
    "           cbar_kws={'label': 'Number of Images'})\n",
    "plt.xlabel('Predicted Digit', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Digit', fontsize=14, fontweight='bold')\n",
    "plt.title('Confusion Matrix\\n(Bright diagonal = Good Performance)', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä How to interpret this:\")\n",
    "print(\"  ‚Ä¢ Each cell shows: 'how many times true digit X was predicted as Y'\")\n",
    "print(\"  ‚Ä¢ Diagonal (top-left to bottom-right) = correct predictions\")\n",
    "print(\"  ‚Ä¢ Bright diagonal with dark off-diagonal = excellent performance!\")\n",
    "print(\"\\nüîç Common confusions (if any):\")\n",
    "\n",
    "# Find most common mistakes\n",
    "mistakes = []\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        if i != j and cm[i, j] > 5:  # More than 5 mistakes\n",
    "            mistakes.append((i, j, cm[i, j]))\n",
    "\n",
    "mistakes.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "if len(mistakes) > 0:\n",
    "    for true_digit, pred_digit, count in mistakes[:5]:\n",
    "        print(f\"  ‚Ä¢ Digit '{true_digit}' confused as '{pred_digit}': {count} times\")\n",
    "    print(\"\\nüí° These confusions make sense! Try writing a 4 and 9 - they can look similar!\")\n",
    "else:\n",
    "    print(\"  ‚Ä¢ Very few mistakes! Excellent performance across all digits!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Visualize What the Network Learned\n",
    "\n",
    "Let's peek inside the hidden layer to see what features it learned to detect!\n",
    "\n",
    "**What you'll see:**\n",
    "- Each hidden neuron has 784 weights (one per pixel)\n",
    "- We can reshape these weights back into 28√ó28 to visualize\n",
    "- Red areas = neuron looks for bright pixels there\n",
    "- Blue areas = neuron looks for dark pixels there\n",
    "- These are the FEATURES the network discovered on its own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hidden layer weights as learned features\n",
    "fig, axes = plt.subplots(8, 16, figsize=(20, 10))\n",
    "fig.suptitle('All 128 Hidden Layer Features (What Each Neuron \"Looks For\")', \n",
    "             fontsize=18, fontweight='bold')\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Get the weights for this hidden neuron and reshape to 28√ó28\n",
    "    weights = nn.W1[:, i].reshape(28, 28)\n",
    "    \n",
    "    ax.imshow(weights, cmap='RdBu', vmin=-1, vmax=1)\n",
    "    ax.set_title(f'#{i}', fontsize=8)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüß† What are we seeing?\")\n",
    "print(\"  ‚Ä¢ Each image = what ONE hidden neuron learned to detect\")\n",
    "print(\"  ‚Ä¢ Red regions = neuron activates when pixels are BRIGHT there\")\n",
    "print(\"  ‚Ä¢ Blue regions = neuron activates when pixels are DARK there\")\n",
    "print(\"  ‚Ä¢ White = neuron doesn't care about those pixels\")\n",
    "print(\"\\nüí° Amazing fact: We NEVER told the network what to look for!\")\n",
    "print(\"   It discovered these features on its own through training!\")\n",
    "print(\"\\nüîç Look for patterns like:\")\n",
    "print(\"  ‚Ä¢ Edge detectors (vertical/horizontal lines)\")\n",
    "print(\"  ‚Ä¢ Curve detectors (for recognizing rounded digits like 0, 6, 8, 9)\")\n",
    "print(\"  ‚Ä¢ Corner detectors\")\n",
    "print(\"  ‚Ä¢ Specific digit patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a specific example: how hidden neurons respond to a specific digit\n",
    "test_idx = 42  # Pick any test image\n",
    "test_image = X_test[test_idx]\n",
    "test_label = y_test_labels[test_idx]\n",
    "\n",
    "# Forward pass to get hidden layer activations\n",
    "_ = nn.forward(test_image.reshape(1, -1))\n",
    "hidden_activations = nn.a1[0]  # Activations of all 128 hidden neurons\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Show the input image\n",
    "axes[0].imshow(test_image.reshape(28, 28), cmap='gray')\n",
    "axes[0].set_title(f'Input Image: Digit {test_label}', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Show hidden layer activations\n",
    "axes[1].bar(range(len(hidden_activations)), hidden_activations, color='steelblue')\n",
    "axes[1].set_xlabel('Hidden Neuron ID (0-127)', fontsize=12)\n",
    "axes[1].set_ylabel('Activation Level', fontsize=12)\n",
    "axes[1].set_title('Hidden Layer Response\\n(How much each neuron activated)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Show output layer probabilities\n",
    "output_probs = nn.a2[0]\n",
    "colors = ['green' if i == test_label else 'lightblue' for i in range(10)]\n",
    "axes[2].bar(range(10), output_probs, color=colors, edgecolor='black', linewidth=2)\n",
    "axes[2].set_xlabel('Digit', fontsize=12)\n",
    "axes[2].set_ylabel('Probability', fontsize=12)\n",
    "axes[2].set_title(f'Output Probabilities\\n(Prediction: {np.argmax(output_probs)})', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[2].set_xticks(range(10))\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for i, prob in enumerate(output_probs):\n",
    "    if prob > 0.01:  # Only show significant probabilities\n",
    "        axes[2].text(i, prob + 0.02, f'{prob:.3f}', ha='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüîç Flow of information for this digit {test_label}:\")\n",
    "print(f\"  1. Input: 784 pixel values (the flattened image)\")\n",
    "print(f\"  2. Hidden: 128 feature detectors activate (middle graph shows which ones)\")\n",
    "print(f\"  3. Output: 10 probabilities computed (right graph)\")\n",
    "print(f\"  4. Prediction: Digit with highest probability = {np.argmax(output_probs)}\")\n",
    "print(f\"\\nüí° Notice:\")\n",
    "print(f\"  ‚Ä¢ Different hidden neurons activate for different digits\")\n",
    "print(f\"  ‚Ä¢ The network is {output_probs[test_label]*100:.1f}% confident this is a {test_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 13: Interactive Prediction - Try Your Own!\n",
    "\n",
    "Let's make predictions on random test images and see the network's reasoning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction function\n",
    "def show_prediction_detail(index):\n",
    "    \"\"\"\n",
    "    Show detailed prediction for a specific test image\n",
    "    \"\"\"\n",
    "    image = X_test[index].reshape(28, 28)\n",
    "    true_label = y_test_labels[index]\n",
    "    \n",
    "    # Get prediction\n",
    "    probs = nn.forward(X_test[index:index+1])[0]\n",
    "    predicted_label = np.argmax(probs)\n",
    "    confidence = probs[predicted_label]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Show image\n",
    "    axes[0].imshow(image, cmap='gray')\n",
    "    axes[0].set_title(f'Test Image #{index}\\nTrue Label: {true_label}', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Show probabilities\n",
    "    colors = ['green' if i == predicted_label else 'lightblue' for i in range(10)]\n",
    "    axes[1].bar(range(10), probs, color=colors, edgecolor='black', linewidth=2)\n",
    "    axes[1].set_xlabel('Digit', fontsize=13)\n",
    "    axes[1].set_ylabel('Probability', fontsize=13)\n",
    "    axes[1].set_title(f'Network Prediction: {predicted_label} (confidence: {confidence:.1%})', \n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xticks(range(10))\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add probability labels\n",
    "    for i, prob in enumerate(probs):\n",
    "        if prob > 0.01:\n",
    "            axes[1].text(i, prob + 0.01, f'{prob:.3f}', ha='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    if predicted_label == true_label:\n",
    "        print(f\"‚úÖ CORRECT! The network correctly identified this as a {true_label}\")\n",
    "        print(f\"   Confidence: {confidence:.2%}\")\n",
    "    else:\n",
    "        print(f\"‚ùå INCORRECT! The network predicted {predicted_label} but it's actually {true_label}\")\n",
    "        print(f\"   Confidence in wrong answer: {confidence:.2%}\")\n",
    "        print(f\"   Confidence in correct answer: {probs[true_label]:.2%}\")\n",
    "    \n",
    "    print(f\"\\nTop 3 predictions:\")\n",
    "    top3_indices = np.argsort(probs)[-3:][::-1]\n",
    "    for rank, idx in enumerate(top3_indices, 1):\n",
    "        print(f\"  {rank}. Digit {idx}: {probs[idx]:.2%}\")\n",
    "\n",
    "# Try a few random examples\n",
    "print(\"üé≤ Let's examine some random predictions:\\n\")\n",
    "for _ in range(3):\n",
    "    random_idx = np.random.randint(0, len(X_test))\n",
    "    show_prediction_detail(random_idx)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary: What You've Learned\n",
    "\n",
    "### Core Concepts Mastered:\n",
    "\n",
    "**1. Image Flattening (THE KEY CONCEPT!)**\n",
    "- 2D images (28√ó28) ‚Üí 1D vectors (784 elements)\n",
    "- Row-by-row concatenation preserves all information\n",
    "- Each pixel value maps to exactly one input neuron\n",
    "\n",
    "**2. Neural Network Architecture**\n",
    "- Input layer: 784 neurons (one per pixel)\n",
    "- Hidden layer: 128 neurons with ReLU activation (feature detectors)\n",
    "- Output layer: 10 neurons with Softmax (probability distribution)\n",
    "- Total: 101,770 trainable parameters\n",
    "\n",
    "**3. The Training Process**\n",
    "- Forward propagation: input ‚Üí hidden ‚Üí output\n",
    "- Loss computation: measuring prediction error\n",
    "- Backpropagation: computing gradients\n",
    "- Gradient descent: updating weights to reduce error\n",
    "\n",
    "**4. Evaluation**\n",
    "- Training set: learn patterns (60,000 images)\n",
    "- Test set: measure generalization (10,000 unseen images)\n",
    "- Achieved >95% accuracy on handwritten digits!\n",
    "\n",
    "### Why This Matters for Your Assignment:\n",
    "\n",
    "**Text Classification uses the SAME principles:**\n",
    "- Text ‚Üí Vector (instead of Image ‚Üí Vector)\n",
    "- Same neural network architecture\n",
    "- Same forward/backward propagation\n",
    "- Same training loop\n",
    "\n",
    "**The simplified assignment gives you:**\n",
    "- Pre-processed text (we handle the \"flattening\" for you)\n",
    "- 500 movie reviews (manageable dataset size)\n",
    "- 3 focused TODOs (forward pass, loss, training)\n",
    "- 2-4 hours (achievable timeframe)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Review this notebook** - make sure you understand flattening!\n",
    "2. **Try the simplified assignment** - apply what you learned\n",
    "3. **Experiment** - change hyperparameters, observe effects\n",
    "4. **Ask questions** - if anything is unclear, ask now!\n",
    "\n",
    "### Key Takeaway:\n",
    "\n",
    "**Neural networks are pattern-matching machines:**\n",
    "- We give them data (images, text, anything!)\n",
    "- We show them examples (training)\n",
    "- They learn to recognize patterns (weights)\n",
    "- They can generalize to new data (testing)\n",
    "\n",
    "**You now understand how they work! üéâ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Bonus: Experiment Ideas\n",
    "\n",
    "Want to explore further? Try these experiments:\n",
    "\n",
    "1. **Change the hidden layer size:**\n",
    "   - Try 64 neurons: Faster but less accurate?\n",
    "   - Try 256 neurons: More accurate but slower?\n",
    "\n",
    "2. **Adjust learning rate:**\n",
    "   - Try 0.01: Slower learning?\n",
    "   - Try 0.5: Faster but unstable?\n",
    "\n",
    "3. **Train for more epochs:**\n",
    "   - Does accuracy keep improving?\n",
    "   - When does it plateau?\n",
    "\n",
    "4. **Analyze mistakes:**\n",
    "   - Find all misclassified images\n",
    "   - Are they hard for humans too?\n",
    "   - What patterns do you notice?\n",
    "\n",
    "5. **Add a second hidden layer:**\n",
    "   - Does deeper = better?\n",
    "   - What's the performance/speed tradeoff?\n",
    "\n",
    "**Happy experimenting! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
