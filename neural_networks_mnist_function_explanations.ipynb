{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Neural Network: Function Explanations\n",
    "\n",
    "## Line-by-Line Breakdown of Core Functions\n",
    "\n",
    "This notebook provides detailed explanations of the four key functions in the MNIST neural network:\n",
    "1. **compute_loss()** - Measuring prediction errors\n",
    "2. **backward()** - Backpropagation & weight updates\n",
    "3. **train()** - The main training loop\n",
    "4. **predict()** - Making predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 1: `compute_loss(self, y_true, y_pred)`\n",
    "\n",
    "**Purpose:** Calculate cross-entropy loss to measure how wrong the predictions are.\n",
    "\n",
    "**Location in code:** Lines 318-323\n",
    "\n",
    "### The Complete Function\n",
    "\n",
    "```python\n",
    "def compute_loss(self, y_true, y_pred):\n",
    "    \"\"\"Cross-entropy loss\"\"\"\n",
    "    m = y_true.shape[0]                                    # Line 320\n",
    "    log_likelihood = -np.log(y_pred[range(m), y_true.argmax(axis=1)])  # Line 321\n",
    "    loss = np.sum(log_likelihood) / m                      # Line 322\n",
    "    return loss                                             # Line 323\n",
    "```\n",
    "\n",
    "### Line-by-Line Breakdown\n",
    "\n",
    "**Line 320: `m = y_true.shape[0]`**\n",
    "- Gets the batch size (number of samples)\n",
    "- If `y_true` has shape (128, 10), then `m = 128`\n",
    "- This tells us how many images we're evaluating\n",
    "\n",
    "**Line 321: `y_true.argmax(axis=1)`**\n",
    "- Converts one-hot encoding to class indices\n",
    "- Example: `[0, 0, 0, 1, 0, ...]` (class 3) → becomes `3`\n",
    "- Does this for all 128 samples\n",
    "\n",
    "**Line 321: `y_pred[range(m), y_true.argmax(axis=1)]`**\n",
    "- **Advanced indexing** (fancy indexing)\n",
    "- For each sample i, gets: `y_pred[i, true_class_i]`\n",
    "- Extracts ONLY the predicted probability for the correct class\n",
    "- Example:\n",
    "  ```\n",
    "  Sample 1: y_pred = [0.1, 0.2, 0.1, 0.6, 0.0, ...]  true_class = 3\n",
    "  → Extract: 0.6 (the probability for class 3)\n",
    "  \n",
    "  Sample 2: y_pred = [0.3, 0.5, 0.1, 0.1, ...]  true_class = 1\n",
    "  → Extract: 0.5 (the probability for class 1)\n",
    "  ```\n",
    "\n",
    "**Line 321: `-np.log(...)`**\n",
    "- Apply negative logarithm to each extracted probability\n",
    "- **Why logarithm?** It heavily penalizes wrong predictions\n",
    "  - If probability = 0.9 (good): loss = -log(0.9) ≈ 0.105 (small)\n",
    "  - If probability = 0.5 (uncertain): loss = -log(0.5) ≈ 0.693 (medium)\n",
    "  - If probability = 0.1 (wrong): loss = -log(0.1) ≈ 2.303 (large)\n",
    "- Result: array of 128 loss values (one per sample)\n",
    "\n",
    "**Line 322: `np.sum(log_likelihood) / m`**\n",
    "- Sum all 128 individual losses\n",
    "- Divide by batch size (128) to get AVERAGE loss\n",
    "- **Why average?** Makes loss independent of batch size (small batches don't artificially have small losses)\n",
    "- Result: single scalar value\n",
    "\n",
    "**Line 323: `return loss`**\n",
    "- Return the average cross-entropy loss\n",
    "- Used to monitor training progress\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "```\n",
    "Input:\n",
    "  y_true (one-hot):  [[0,0,1,0], [1,0,0,0]]  # True classes: 2 and 0\n",
    "  y_pred (softmax):  [[0.1, 0.2, 0.6, 0.1], [0.8, 0.1, 0.05, 0.05]]\n",
    "\n",
    "Step 1: Get true classes\n",
    "  y_true.argmax(axis=1) → [2, 0]\n",
    "\n",
    "Step 2: Extract predictions for true classes\n",
    "  y_pred[0, 2] = 0.6\n",
    "  y_pred[1, 0] = 0.8\n",
    "  Result: [0.6, 0.8]\n",
    "\n",
    "Step 3: Apply negative log\n",
    "  -log(0.6) ≈ 0.51\n",
    "  -log(0.8) ≈ 0.22\n",
    "  Result: [0.51, 0.22]\n",
    "\n",
    "Step 4: Average\n",
    "  (0.51 + 0.22) / 2 = 0.365\n",
    "\n",
    "Output: 0.365 (average loss)\n",
    "```\n",
    "\n",
    "### What It Means\n",
    "- **Lower loss = better predictions**\n",
    "- Loss 0.1 = very confident and correct ✓✓✓\n",
    "- Loss 0.5 = somewhat confident and correct ✓✓\n",
    "- Loss 2.0 = very confident but WRONG ✗✗✗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 2: `backward(self, X, y_true, learning_rate=0.01)`\n",
    "\n",
    "**Purpose:** Compute gradients via backpropagation and update all network weights.\n",
    "\n",
    "**Location in code:** Lines 325-343\n",
    "\n",
    "### The Complete Function\n",
    "\n",
    "```python\n",
    "def backward(self, X, y_true, learning_rate=0.01):\n",
    "    \"\"\"Backpropagation\"\"\"\n",
    "    m = X.shape[0]                                        # Line 327\n",
    "    \n",
    "    # Output layer gradients\n",
    "    dz2 = self.a2 - y_true                               # Line 330\n",
    "    dW2 = np.dot(self.a1.T, dz2) / m                     # Line 331\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True) / m         # Line 332\n",
    "    \n",
    "    # Hidden layer gradients\n",
    "    dz1 = np.dot(dz2, self.W2.T) * relu_derivative(self.z1)  # Line 335\n",
    "    dW1 = np.dot(X.T, dz1) / m                           # Line 336\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True) / m         # Line 337\n",
    "    \n",
    "    # Update weights\n",
    "    self.W2 -= learning_rate * dW2                       # Line 340\n",
    "    self.b2 -= learning_rate * db2                       # Line 341\n",
    "    self.W1 -= learning_rate * dW1                       # Line 342\n",
    "    self.b1 -= learning_rate * db1                       # Line 343\n",
    "```\n",
    "\n",
    "### Line-by-Line Breakdown\n",
    "\n",
    "**Line 327: `m = X.shape[0]`**\n",
    "- Batch size (e.g., 128)\n",
    "- Used to normalize gradients\n",
    "\n",
    "#### OUTPUT LAYER (Working Backwards)\n",
    "\n",
    "**Line 330: `dz2 = self.a2 - y_true`**\n",
    "- **Key line!** Compute error at output layer\n",
    "- `self.a2` = predictions from softmax, shape (128, 10)\n",
    "- `y_true` = one-hot labels, shape (128, 10)\n",
    "- Result: (128, 10) matrix of errors\n",
    "- This is the **gradient of loss w.r.t. output layer** (thanks to softmax + cross-entropy)\n",
    "- Example: if prediction [0.1, 0.9] and truth [0, 1], error = [0.1, -0.1]\n",
    "\n",
    "**Line 331: `dW2 = np.dot(self.a1.T, dz2) / m`**\n",
    "- Compute gradient for hidden→output weights\n",
    "- `self.a1.T` = hidden layer activations transposed, shape (128, 128) → (128, 128)\n",
    "- Matrix multiplication: (128, 128) × (128, 10) = (128, 10) **wait** should be (128, 10) ✓\n",
    "- **What it means:** How much each hidden activation contributed to the output error\n",
    "- Divide by m: normalize per sample\n",
    "\n",
    "**Line 332: `db2 = np.sum(dz2, axis=0, keepdims=True) / m`**\n",
    "- Compute gradient for output bias\n",
    "- Sum errors across all samples (axis=0): (128, 10) → (1, 10)\n",
    "- Each output neuron gets one bias gradient value\n",
    "- Divide by m: average per sample\n",
    "\n",
    "#### HIDDEN LAYER (Backpropagate Further)\n",
    "\n",
    "**Line 335: `np.dot(dz2, self.W2.T)`**\n",
    "- **Propagate error backwards through W2**\n",
    "- Take output layer errors and \"spread them back\" to hidden layer\n",
    "- (128, 10) × (10, 128) = (128, 128)\n",
    "- This tells us: \"how much did each hidden neuron contribute to the output error?\"\n",
    "\n",
    "**Line 335: `* relu_derivative(self.z1)`**\n",
    "- **Apply ReLU derivative**\n",
    "- For each hidden neuron: if z1 < 0, gradient is 0 (dead neuron, no contribution)\n",
    "- If z1 > 0, gradient passes through (coefficient = 1)\n",
    "- This respects the ReLU activation function\n",
    "- Result: (128, 128) hidden layer gradient\n",
    "\n",
    "**Line 336: `dW1 = np.dot(X.T, dz1) / m`**\n",
    "- Compute gradient for input→hidden weights\n",
    "- `X.T` = input transposed, shape (784, 128)\n",
    "- Matrix multiplication: (784, 128) × (128, 128) = (784, 128) ✓\n",
    "- This tells us: \"how much does each input pixel affect the hidden layer error?\"\n",
    "\n",
    "**Line 337: `db1 = np.sum(dz1, axis=0, keepdims=True) / m`**\n",
    "- Compute gradient for hidden bias\n",
    "- Sum across samples: (128, 128) → (1, 128)\n",
    "\n",
    "#### WEIGHT UPDATES (Gradient Descent)\n",
    "\n",
    "**Lines 340-343: Update all weights and biases**\n",
    "```python\n",
    "self.W2 -= learning_rate * dW2    # Move opposite to gradient\n",
    "self.b2 -= learning_rate * db2\n",
    "self.W1 -= learning_rate * dW1\n",
    "self.b1 -= learning_rate * db1\n",
    "```\n",
    "- **Gradient descent step**: Move in direction that reduces loss\n",
    "- `learning_rate` controls step size (e.g., 0.1)\n",
    "- Large gradient = large update\n",
    "- Small gradient = small update\n",
    "- All 4 parameter matrices are updated simultaneously\n",
    "\n",
    "### Visual Flow\n",
    "\n",
    "```\n",
    "Forward Pass (already done):\n",
    "  X (128, 784) → W1 → z1 → ReLU → a1 (128, 128) → W2 → z2 → Softmax → a2 (128, 10)\n",
    "\n",
    "Backward Pass (what backward() does):\n",
    "  Error at output: dz2 = a2 - y_true (128, 10)\n",
    "       ↓\n",
    "  Compute dW2, db2 (gradients for W2, b2)\n",
    "       ↓\n",
    "  Backprop to hidden: dz1 = dz2 @ W2.T * relu_derivative (128, 128)\n",
    "       ↓\n",
    "  Compute dW1, db1 (gradients for W1, b1)\n",
    "       ↓\n",
    "  Update all weights: W1 -= lr * dW1, etc.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 3: `train(self, X_train, y_train, X_test, y_test, epochs=10, batch_size=128, learning_rate=0.1)`\n",
    "\n",
    "**Purpose:** Main training loop that orchestrates everything.\n",
    "\n",
    "**Location in code:** Lines 345-384\n",
    "\n",
    "### The Complete Function (Simplified Structure)\n",
    "\n",
    "```python\n",
    "def train(self, X_train, y_train, X_test, y_test, epochs=10, batch_size=128, learning_rate=0.1):\n",
    "    n_batches = len(X_train) // batch_size              # Line 347\n",
    "    \n",
    "    for epoch in range(epochs):                         # Line 349\n",
    "        # Shuffle training data\n",
    "        indices = np.random.permutation(len(X_train))   # Line 351\n",
    "        X_shuffled = X_train[indices]                   # Line 352\n",
    "        y_shuffled = y_train[indices]                   # Line 353\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for i in range(n_batches):                      # Line 356\n",
    "            start_idx = i * batch_size                  # Line 357\n",
    "            end_idx = start_idx + batch_size            # Line 358\n",
    "            \n",
    "            X_batch = X_shuffled[start_idx:end_idx]     # Line 360\n",
    "            y_batch = y_shuffled[start_idx:end_idx]     # Line 361\n",
    "            \n",
    "            # Forward and backward pass\n",
    "            self.forward(X_batch)                       # Line 364\n",
    "            self.backward(X_batch, y_batch, learning_rate)  # Line 365\n",
    "        \n",
    "        # Evaluate on full sets\n",
    "        train_pred = self.forward(X_train)              # Line 368\n",
    "        test_pred = self.forward(X_test)                # Line 369\n",
    "        \n",
    "        train_loss = self.compute_loss(y_train, train_pred)   # Line 371\n",
    "        test_loss = self.compute_loss(y_test, test_pred)      # Line 372\n",
    "        \n",
    "        # Compute accuracy\n",
    "        train_acc = np.mean(...)  # Line 374\n",
    "        test_acc = np.mean(...)   # Line 375\n",
    "        \n",
    "        # Store and print\n",
    "        self.loss_history.append(test_loss)             # Line 377\n",
    "        self.accuracy_history.append(test_acc)          # Line 378\n",
    "        print(...)                                       # Line 380\n",
    "```\n",
    "\n",
    "### Line-by-Line Breakdown\n",
    "\n",
    "**Line 347: `n_batches = len(X_train) // batch_size`**\n",
    "- Calculate number of batches per epoch\n",
    "- With 60,000 images and batch_size=128: 60000 // 128 = 468 batches\n",
    "- Each epoch will have 468 update steps\n",
    "\n",
    "**Line 349: `for epoch in range(epochs):`**\n",
    "- Main training loop: repeat 10 times (for epochs=10)\n",
    "- Each epoch = one complete pass through all training data\n",
    "- In each epoch, weights get updated 468 times (once per batch)\n",
    "\n",
    "**Line 351: `indices = np.random.permutation(len(X_train))`**\n",
    "- Create random ordering of sample indices\n",
    "- Example: [45, 120, 3, 99, 5, ...] (random order)\n",
    "- **Why shuffle?** \n",
    "  - Prevents overfitting to data order\n",
    "  - Helps gradient descent explore solution space better\n",
    "  - Each epoch sees data in different order\n",
    "\n",
    "**Lines 352-353: Reorder training data**\n",
    "```python\n",
    "X_shuffled = X_train[indices]  # Reorder images\n",
    "y_shuffled = y_train[indices]  # Reorder labels (keep alignment!)\n",
    "```\n",
    "- Apply shuffled indices to both images and labels\n",
    "- Maintains alignment: image[i] still matches label[i]\n",
    "\n",
    "**Line 356: `for i in range(n_batches):`**\n",
    "- Loop through all 468 batches\n",
    "- Each iteration processes 128 samples\n",
    "\n",
    "**Lines 357-358: Calculate batch boundaries**\n",
    "```python\n",
    "start_idx = i * batch_size      # Batch 0: 0, Batch 1: 128, Batch 2: 256, ...\n",
    "end_idx = start_idx + batch_size  # Batch 0: 128, Batch 1: 256, ...\n",
    "```\n",
    "\n",
    "**Lines 360-361: Extract batch**\n",
    "```python\n",
    "X_batch = X_shuffled[start_idx:end_idx]  # 128 images\n",
    "y_batch = y_shuffled[start_idx:end_idx]  # 128 labels\n",
    "```\n",
    "\n",
    "**Line 364: `self.forward(X_batch)`**\n",
    "- Forward pass on 128 images\n",
    "- Computes predictions (self.a2)\n",
    "\n",
    "**Line 365: `self.backward(X_batch, y_batch, learning_rate)`**\n",
    "- Backward pass: compute gradients for this batch\n",
    "- Update weights based on these 128 samples\n",
    "\n",
    "**Lines 368-369: Evaluate after epoch**\n",
    "```python\n",
    "train_pred = self.forward(X_train)  # Predict on ALL 60K training images\n",
    "test_pred = self.forward(X_test)    # Predict on ALL 10K test images\n",
    "```\n",
    "- After processing all 468 batches, evaluate on complete datasets\n",
    "- Test accuracy shows generalization\n",
    "\n",
    "**Lines 371-372: Compute loss**\n",
    "```python\n",
    "train_loss = self.compute_loss(y_train, train_pred)  # Loss on training set\n",
    "test_loss = self.compute_loss(y_test, test_pred)    # Loss on test set\n",
    "```\n",
    "\n",
    "**Lines 374-375: Compute accuracy**\n",
    "```python\n",
    "train_acc = np.mean(np.argmax(train_pred, axis=1) == np.argmax(y_train, axis=1))\n",
    "test_acc = np.mean(np.argmax(test_pred, axis=1) == np.argmax(y_test, axis=1))\n",
    "```\n",
    "- Convert predictions to class indices\n",
    "- Compare with true class indices\n",
    "- Compute % correct\n",
    "\n",
    "**Lines 377-378: Store history**\n",
    "```python\n",
    "self.loss_history.append(test_loss)      # Save for plotting\n",
    "self.accuracy_history.append(test_acc)   # Save for plotting\n",
    "```\n",
    "\n",
    "### Training Flow Example\n",
    "\n",
    "```\n",
    "Epoch 1:\n",
    "  Shuffle 60K images\n",
    "  Batch 1: Process images 0-127, update weights\n",
    "  Batch 2: Process images 128-255, update weights\n",
    "  ...\n",
    "  Batch 468: Process images 59776-59903, update weights\n",
    "  → Evaluate: Train loss=0.8, Test loss=0.9, Train acc=78%, Test acc=76%\n",
    "\n",
    "Epoch 2:\n",
    "  Shuffle 60K images (different order!)\n",
    "  Batch 1-468: Same process with new order\n",
    "  → Evaluate: Train loss=0.5, Test loss=0.6, Train acc=85%, Test acc=84%\n",
    "\n",
    "...(repeat until epoch 10)...\n",
    "\n",
    "Epoch 10:\n",
    "  Shuffle and process all batches\n",
    "  → Evaluate: Train loss=0.2, Test loss=0.3, Train acc=96%, Test acc=95%\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function 4: `predict(self, X)`\n",
    "\n",
    "**Purpose:** Make predictions on new data.\n",
    "\n",
    "**Location in code:** Lines 386-389\n",
    "\n",
    "### The Complete Function\n",
    "\n",
    "```python\n",
    "def predict(self, X):\n",
    "    \"\"\"Make predictions\"\"\"\n",
    "    probabilities = self.forward(X)              # Line 388\n",
    "    return np.argmax(probabilities, axis=1)      # Line 389\n",
    "```\n",
    "\n",
    "### Line-by-Line Breakdown\n",
    "\n",
    "**Line 388: `probabilities = self.forward(X)`**\n",
    "- Run forward pass on input X\n",
    "- Input X shape: (n_samples, 784) - e.g., (100, 784) for 100 test images\n",
    "- Output shape: (n_samples, 10) - probabilities for each class\n",
    "- Each row contains 10 probabilities that sum to 1\n",
    "- Example:\n",
    "  ```\n",
    "  Sample 1: [0.01, 0.05, 0.02, 0.88, 0.01, 0.00, 0.01, 0.01, 0.01, 0.00]\n",
    "  Sample 2: [0.02, 0.92, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.00, 0.00]\n",
    "  Sample 3: [0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10]\n",
    "  ```\n",
    "  - Sample 1 is most confident it's a 3 (88% probability)\n",
    "  - Sample 2 is most confident it's a 1 (92% probability)\n",
    "  - Sample 3 is uncertain about everything (10% each)\n",
    "\n",
    "**Line 389: `np.argmax(probabilities, axis=1)`**\n",
    "- **argmax** = \"find the index of the maximum value\"\n",
    "- For each row (axis=1), find which column has the highest probability\n",
    "- Example:\n",
    "  ```\n",
    "  [0.01, 0.05, 0.02, 0.88, 0.01, ...] → 3 (highest at index 3)\n",
    "  [0.02, 0.92, 0.01, 0.01, 0.01, ...] → 1 (highest at index 1)\n",
    "  [0.10, 0.10, 0.10, 0.10, 0.10, ...] → 0 (first maximum at index 0)\n",
    "  ```\n",
    "- Output shape: (n_samples,) - single predicted class per sample\n",
    "- Example output: [3, 1, 0, ...]\n",
    "\n",
    "**Line 389: `return ...`**\n",
    "- Return array of predicted class indices\n",
    "\n",
    "### Why Two Steps?\n",
    "\n",
    "**Why not just return probabilities?**\n",
    "- `forward()` gives probabilities (soft output): [0.1, 0.2, 0.7]\n",
    "- We need class labels (hard output): 2\n",
    "- `argmax` converts from \"how confident?\" to \"which class?\"\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "```\n",
    "Input: 5 test images\n",
    "  X shape: (5, 784)\n",
    "\n",
    "After forward():\n",
    "  probabilities = [\n",
    "    [0.0, 0.0, 0.05, 0.90, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05],  # Probably 3\n",
    "    [0.0, 0.95, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05],   # Probably 1\n",
    "    [0.2, 0.2, 0.2, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0],      # Uncertain\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.98, 0.02],    # Probably 8\n",
    "    [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.50]  # Probably 9\n",
    "  ]\n",
    "\n",
    "After argmax(axis=1):\n",
    "  predictions = [3, 1, 0, 8, 9]\n",
    "  (Pick the class with highest probability for each sample)\n",
    "```\n",
    "\n",
    "### Common Use Cases\n",
    "\n",
    "```python\n",
    "# Make predictions on test set\n",
    "predictions = nn.predict(X_test)  # Returns [3, 1, 5, 2, ...]\n",
    "\n",
    "# Compare with true labels\n",
    "accuracy = accuracy_score(y_test_labels, predictions)\n",
    "\n",
    "# Predict on single image\n",
    "new_image = X_test[0:1]  # Shape: (1, 784)\n",
    "pred = nn.predict(new_image)  # Returns [5]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table\n",
    "\n",
    "| Function | Input | Output | Purpose | When Called |\n",
    "|----------|-------|--------|---------|-------------|\n",
    "| `compute_loss()` | Predictions (n, 10)<br/>True labels (n, 10) | Scalar loss value | Measure prediction error | During training evaluation |\n",
    "| `backward()` | Batch images (128, 784)<br/>Batch labels (128, 10) | None<br/>(updates weights) | Backprop + weight update | Every batch (468 times/epoch) |\n",
    "| `train()` | 60K train imgs<br/>60K train labels<br/>10K test imgs<br/>10K test labels | None<br/>(stores history) | Run complete training | Once at start |\n",
    "| `predict()` | Test images (n, 784) | Class indices (n,) | Convert probabilities to predictions | After training, for evaluation |\n",
    "\n",
    "---\n",
    "\n",
    "## The Complete Pipeline\n",
    "\n",
    "```\n",
    "1. nn = DigitRecognitionNetwork()  # Create network\n",
    "   \n",
    "2. nn.train(X_train, y_train, X_test, y_test, epochs=10)\n",
    "   ├─ For each epoch:\n",
    "   │  ├─ Shuffle data\n",
    "   │  └─ For each batch:\n",
    "   │     ├─ forward() → compute predictions\n",
    "   │     └─ backward() → compute gradients, update weights\n",
    "   ├─ compute_loss() → track training progress\n",
    "   └─ Print accuracy and loss\n",
    "   \n",
    "3. predictions = nn.predict(X_test)  # Make predictions\n",
    "   ├─ forward() → get probabilities\n",
    "   └─ argmax() → convert to class indices\n",
    "   \n",
    "4. accuracy = accuracy_score(y_test_labels, predictions)  # Evaluate\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
