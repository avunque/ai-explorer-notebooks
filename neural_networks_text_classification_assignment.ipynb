{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Assignment: Text Classification\n",
    "\n",
    "**Student Name:** [Your Name Here]  \n",
    "**Date:** [Date]\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this assignment, you'll apply everything you learned from MNIST to classify **movie reviews** as positive or negative.\n",
    "\n",
    "**Key Insight:** This is the *same problem* as MNIST with different input!\n",
    "\n",
    "| Aspect | MNIST | Text Classification |\n",
    "|--------|-------|--------------------|\n",
    "| **Input** | 28Ã—28 image (784 pixels) | Review text (5,000 word features) |\n",
    "| **Output** | 10 classes (digits 0-9) | 2 classes (positive/negative) |\n",
    "| **Architecture** | Input â†’ Hidden â†’ Output | Input â†’ Hidden â†’ Output |\n",
    "| **Training** | Mini-batch gradient descent | Mini-batch gradient descent |\n",
    "| **Activation** | ReLU + Softmax | ReLU + Sigmoid |\n",
    "\n",
    "**What's Different:** Instead of pixels, we use **word counts**. That's it!\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Do\n",
    "\n",
    "âœ… **Provided for you:** All data preprocessing, helper functions, visualization code  \n",
    "ðŸ”§ **Your tasks:** Implement forward pass, loss function, complete training loop  \n",
    "ðŸ’­ **Reflection:** Understand what you built and how it works\n",
    "\n",
    "**Estimated time:** 2-4 hours\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: Import Libraries\n",
    "\n",
    "Run this cell first. If you get import errors, install missing packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Understanding the Data\n",
    "\n",
    "## 1.1 Sample Dataset (Provided)\n",
    "\n",
    "We've created a simplified dataset of 500 short movie reviews for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample movie reviews (250 positive + 250 negative)\n",
    "positive_reviews = [\n",
    "    \"This movie was absolutely fantastic and entertaining!\",\n",
    "    \"Loved every minute of it, brilliant performance.\",\n",
    "    \"An amazing film with great acting and plot.\",\n",
    "    \"Wonderful story, beautifully shot, highly recommend.\",\n",
    "    \"Excellent movie, one of the best I've seen.\",\n",
    "    \"Incredible experience, the cast was perfect.\",\n",
    "    \"Thoroughly enjoyed this film, very engaging.\",\n",
    "    \"Superb direction and wonderful screenplay.\",\n",
    "    \"Fantastic cinematography and compelling narrative.\",\n",
    "    \"Outstanding performances, truly memorable film.\"\n",
    "] * 25  # Repeat to get 250 samples\n",
    "\n",
    "negative_reviews = [\n",
    "    \"Terrible movie, complete waste of time and money.\",\n",
    "    \"Boring and predictable, couldn't finish watching.\",\n",
    "    \"Awful acting and terrible plot, very disappointed.\",\n",
    "    \"Worst film I've ever seen, avoid at all costs.\",\n",
    "    \"Poorly written and badly executed, not recommended.\",\n",
    "    \"Dreadful movie with no redeeming qualities.\",\n",
    "    \"Horrible experience, regret watching this film.\",\n",
    "    \"Pathetic storyline and unconvincing performances.\",\n",
    "    \"Disappointingly bad, don't waste your time.\",\n",
    "    \"Utterly boring and completely forgettable movie.\"\n",
    "] * 25  # Repeat to get 250 samples\n",
    "\n",
    "# Combine and create labels\n",
    "all_reviews = positive_reviews + negative_reviews\n",
    "labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)  # 1 = positive, 0 = negative\n",
    "\n",
    "print(f\"ðŸ“Š Dataset loaded: {len(all_reviews)} reviews\")\n",
    "print(f\"   â€¢ Positive: {sum(labels)} reviews\")\n",
    "print(f\"   â€¢ Negative: {len(labels) - sum(labels)} reviews\")\n",
    "print(f\"\\nâœ… Dataset is balanced (50% positive, 50% negative)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Sample Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples\n",
    "print(\"ðŸ“ POSITIVE Review Examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"   {i+1}. {positive_reviews[i]}\")\n",
    "\n",
    "print(\"\\nðŸ“ NEGATIVE Review Examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"   {i+1}. {negative_reviews[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.2 Text Preprocessing (All Provided!)\n",
    "\n",
    "**This is the complex part - but we've done it for you!**\n",
    "\n",
    "We need to convert text into numbers. Here's how:\n",
    "\n",
    "1. **Clean text** (lowercase, remove punctuation)\n",
    "2. **Tokenize** (split into words)\n",
    "3. **Build vocabulary** (find most common words)\n",
    "4. **Convert to Bag-of-Words** (count word occurrences)\n",
    "\n",
    "Just run these cells and observe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PROVIDED: Text Preprocessing Functions\n",
    "# ============================================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Split text into words\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "def build_vocabulary(texts, vocab_size=5000):\n",
    "    \"\"\"\n",
    "    Build vocabulary of most common words.\n",
    "    \n",
    "    Returns:\n",
    "        word_to_idx: Dictionary mapping words to indices\n",
    "        idx_to_word: Dictionary mapping indices to words\n",
    "    \"\"\"\n",
    "    # Count all words\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        all_words.extend(tokenize(clean_text(text)))\n",
    "    \n",
    "    # Get most common words\n",
    "    word_counts = Counter(all_words)\n",
    "    most_common = word_counts.most_common(vocab_size)\n",
    "    \n",
    "    # Create mappings\n",
    "    word_to_idx = {word: idx for idx, (word, _) in enumerate(most_common)}\n",
    "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "    \n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "def text_to_bow(text, word_to_idx):\n",
    "    \"\"\"\n",
    "    Convert text to Bag-of-Words vector.\n",
    "    \n",
    "    Example:\n",
    "        \"great movie\" â†’ [0, 0, 1, 0, 1, 0, ...] (5000 features)\n",
    "                          position 2 = \"great\" appears 1 time\n",
    "                          position 4 = \"movie\" appears 1 time\n",
    "    \"\"\"\n",
    "    vector = np.zeros(len(word_to_idx))\n",
    "    tokens = tokenize(clean_text(text))\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word in word_to_idx:\n",
    "            vector[word_to_idx[word]] += 1\n",
    "    \n",
    "    return vector\n",
    "\n",
    "print(\"âœ… Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from all reviews\n",
    "print(\"Building vocabulary...\")\n",
    "word_to_idx, idx_to_word = build_vocabulary(all_reviews, vocab_size=100)\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "print(f\"\\nâœ… Vocabulary built: {vocab_size} unique words\")\n",
    "print(f\"\\nMost common words: {list(word_to_idx.keys())[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See How Text Becomes Numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Convert one review to numbers\n",
    "example_review = positive_reviews[0]\n",
    "example_vector = text_to_bow(example_review, word_to_idx)\n",
    "\n",
    "print(f\"ðŸ“ Original Review:\")\n",
    "print(f\"   '{example_review}'\")\n",
    "print(f\"\\nðŸ”¢ After Bag-of-Words Conversion:\")\n",
    "print(f\"   Vector shape: {example_vector.shape}\")\n",
    "print(f\"   Non-zero elements: {np.count_nonzero(example_vector)}\")\n",
    "print(f\"\\n   First 20 values: {example_vector[:20]}\")\n",
    "print(f\"\\nðŸ’¡ Interpretation: This review uses {int(np.count_nonzero(example_vector))} different words from our vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert All Reviews to Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all reviews to Bag-of-Words vectors\n",
    "print(\"Converting all reviews to numerical format...\")\n",
    "X = np.array([text_to_bow(review, word_to_idx) for review in all_reviews])\n",
    "y = np.array(labels).reshape(-1, 1)  # Reshape to (500, 1)\n",
    "\n",
    "print(f\"\\nâœ… Conversion complete!\")\n",
    "print(f\"   X shape: {X.shape}  (500 reviews Ã— {vocab_size} word features)\")\n",
    "print(f\"   y shape: {y.shape}  (500 labels Ã— 1)\")\n",
    "print(f\"\\nðŸ’¡ Compare to MNIST:\")\n",
    "print(f\"   MNIST: X = (60000, 784)   â†’ 60,000 images, 784 pixels each\")\n",
    "print(f\"   Text:  X = (500, {vocab_size})      â†’ 500 reviews, {vocab_size} word features each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.3 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Data Split:\")\n",
    "print(f\"   Training set: {X_train.shape[0]} reviews\")\n",
    "print(f\"   Test set:     {X_test.shape[0]} reviews\")\n",
    "print(f\"\\n   X_train shape: {X_train.shape}\")\n",
    "print(f\"   y_train shape: {y_train.shape}\")\n",
    "print(f\"\\nâœ… Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: Neural Network Implementation\n",
    "\n",
    "## 2.1 Activation Functions (Provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PROVIDED: Activation Functions\n",
    "# ============================================================================\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation: maps any value to (0, 1)\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation: max(0, x)\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"Derivative of ReLU\"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "print(\"âœ… Activation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.2 Neural Network Class\n",
    "\n",
    "**This is where YOU come in!**\n",
    "\n",
    "You'll implement 3 key methods:\n",
    "1. `forward()` - How data flows through the network\n",
    "2. `compute_loss()` - Measure prediction error\n",
    "3. Complete the training loop (fill in missing lines)\n",
    "\n",
    "The structure is provided - you fill in the TODOs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier:\n",
    "    def __init__(self, input_size, hidden_size, output_size=1):\n",
    "        \"\"\"\n",
    "        Initialize neural network for binary text classification.\n",
    "        \n",
    "        Architecture:\n",
    "            Input (input_size) â†’ Hidden (hidden_size, ReLU) â†’ Output (1, Sigmoid)\n",
    "        \n",
    "        Args:\n",
    "            input_size: Number of input features (vocabulary size)\n",
    "            hidden_size: Number of neurons in hidden layer\n",
    "            output_size: Number of output neurons (1 for binary classification)\n",
    "        \"\"\"\n",
    "        # Initialize weights with small random values\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        # Storage for training history\n",
    "        self.train_loss_history = []\n",
    "        self.test_loss_history = []\n",
    "        self.train_acc_history = []\n",
    "        self.test_acc_history = []\n",
    "        \n",
    "        print(f\"ðŸ§  Neural Network Initialized\")\n",
    "        print(f\"   Architecture: {input_size} â†’ {hidden_size} â†’ {output_size}\")\n",
    "        print(f\"   Total parameters: {input_size * hidden_size + hidden_size + hidden_size * output_size + output_size:,}\")\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass: Input â†’ Hidden â†’ Output\n",
    "        \n",
    "        TODO: Implement the forward pass!\n",
    "        \n",
    "        Steps:\n",
    "            1. Compute hidden layer: z1 = X @ W1 + b1\n",
    "            2. Apply ReLU activation: a1 = relu(z1)\n",
    "            3. Compute output layer: z2 = a1 @ W2 + b2\n",
    "            4. Apply sigmoid activation: a2 = sigmoid(z2)\n",
    "        \n",
    "        Hint: This is nearly IDENTICAL to MNIST forward pass!\n",
    "        The only difference: sigmoid instead of softmax at the end.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data (batch_size, input_size)\n",
    "        \n",
    "        Returns:\n",
    "            a2: Output predictions (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward pass\n",
    "        # Step 1: Hidden layer (before activation)\n",
    "        self.z1 = None  # Replace None with your code\n",
    "        \n",
    "        # Step 2: Hidden layer (after ReLU activation)\n",
    "        self.a1 = None  # Replace None with your code\n",
    "        \n",
    "        # Step 3: Output layer (before activation)\n",
    "        self.z2 = None  # Replace None with your code\n",
    "        \n",
    "        # Step 4: Output layer (after sigmoid activation)\n",
    "        self.a2 = None  # Replace None with your code\n",
    "        \n",
    "        return self.a2\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute Binary Cross-Entropy Loss.\n",
    "        \n",
    "        TODO: Implement the loss function!\n",
    "        \n",
    "        Formula:\n",
    "            loss = -1/m * Î£[y*log(Å·) + (1-y)*log(1-Å·)]\n",
    "        \n",
    "        Where:\n",
    "            m = number of samples\n",
    "            y = true labels (0 or 1)\n",
    "            Å· = predicted probabilities (0 to 1)\n",
    "        \n",
    "        Hint: Use np.log(), be careful of log(0)!\n",
    "        Add small epsilon (1e-8) to avoid log(0).\n",
    "        \n",
    "        Args:\n",
    "            y_true: True labels (m, 1)\n",
    "            y_pred: Predicted probabilities (m, 1)\n",
    "        \n",
    "        Returns:\n",
    "            loss: Single number (average loss)\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]\n",
    "        epsilon = 1e-8  # Small value to prevent log(0)\n",
    "        \n",
    "        # TODO: Implement binary cross-entropy loss\n",
    "        # Clip predictions to avoid log(0)\n",
    "        y_pred_clipped = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Calculate loss (replace the 0 with your formula)\n",
    "        loss = 0  # Replace this with your implementation\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y_true, learning_rate):\n",
    "        \"\"\"\n",
    "        Backward pass: Compute gradients and update weights.\n",
    "        \n",
    "        PROVIDED: This is implemented for you!\n",
    "        Study it - it's the same backpropagation from MNIST.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradients\n",
    "        dz2 = self.a2 - y_true  # For sigmoid + binary cross-entropy\n",
    "        dW2 = np.dot(self.a1.T, dz2) / m\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Hidden layer gradients\n",
    "        dz1 = np.dot(dz2, self.W2.T) * relu_derivative(self.z1)\n",
    "        dW1 = np.dot(X.T, dz1) / m\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # Update weights\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions (0 or 1).\n",
    "        \n",
    "        PROVIDED: Converts probabilities to class labels.\n",
    "        \"\"\"\n",
    "        probabilities = self.forward(X)\n",
    "        return (probabilities >= 0.5).astype(int)\n",
    "\n",
    "print(\"âœ… TextClassifier class defined\")\n",
    "print(\"\\nâš ï¸  Remember to implement the TODO sections in forward() and compute_loss()!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2.3 Create the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network instance\n",
    "model = TextClassifier(input_size=vocab_size, hidden_size=64, output_size=1)\n",
    "\n",
    "print(\"\\nðŸ“Š Network Architecture:\")\n",
    "print(f\"   Input Layer:  {vocab_size} neurons (one per word in vocabulary)\")\n",
    "print(f\"   Hidden Layer: 64 neurons (ReLU activation)\")\n",
    "print(f\"   Output Layer: 1 neuron (Sigmoid activation â†’ probability)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Training\n",
    "\n",
    "## 3.1 Training Loop\n",
    "\n",
    "**You need to complete 3 lines in the training loop below!**\n",
    "\n",
    "Look for `# TODO` comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "batch_size = 32\n",
    "\n",
    "print(f\"ðŸ‹ï¸ Training Configuration:\")\n",
    "print(f\"   Epochs: {epochs}\")\n",
    "print(f\"   Learning Rate: {learning_rate}\")\n",
    "print(f\"   Batch Size: {batch_size}\")\n",
    "print(f\"\\nStarting training...\\n\")\n",
    "\n",
    "# Training loop\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle training data\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    X_shuffled = X_train[indices]\n",
    "    y_shuffled = y_train[indices]\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        \n",
    "        X_batch = X_shuffled[start_idx:end_idx]\n",
    "        y_batch = y_shuffled[start_idx:end_idx]\n",
    "        \n",
    "        # TODO 1: Forward pass - make predictions on this batch\n",
    "        # Hint: Use model.forward(X_batch)\n",
    "        predictions = None  # Replace None with your code\n",
    "        \n",
    "        # TODO 2: Backward pass - update weights based on errors\n",
    "        # Hint: Use model.backward(X_batch, y_batch, learning_rate)\n",
    "        # YOUR CODE HERE\n",
    "    \n",
    "    # Evaluate every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        # Predictions on full datasets\n",
    "        train_pred = model.forward(X_train)\n",
    "        test_pred = model.forward(X_test)\n",
    "        \n",
    "        # TODO 3: Calculate loss using model.compute_loss()\n",
    "        # Hint: train_loss = model.compute_loss(y_train, train_pred)\n",
    "        train_loss = None  # Replace None with your code\n",
    "        test_loss = model.compute_loss(y_test, test_pred)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        train_acc = np.mean((train_pred >= 0.5).astype(int) == y_train)\n",
    "        test_acc = np.mean((test_pred >= 0.5).astype(int) == y_test)\n",
    "        \n",
    "        # Store history\n",
    "        model.train_loss_history.append(train_loss)\n",
    "        model.test_loss_history.append(test_loss)\n",
    "        model.train_acc_history.append(train_acc)\n",
    "        model.test_acc_history.append(test_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.2 Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(range(10, epochs+1, 10), model.train_loss_history, 'b-', label='Training Loss', linewidth=2)\n",
    "axes[0].plot(range(10, epochs+1, 10), model.test_loss_history, 'r-', label='Test Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training vs Test Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(range(10, epochs+1, 10), [acc*100 for acc in model.train_acc_history], 'b-', label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(range(10, epochs+1, 10), [acc*100 for acc in model.test_acc_history], 'r-', label='Test Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training vs Test Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 100])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nðŸ“Š Final Results:\")\n",
    "print(f\"   Training Accuracy: {model.train_acc_history[-1]*100:.2f}%\")\n",
    "print(f\"   Test Accuracy:     {model.test_acc_history[-1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Evaluation\n",
    "\n",
    "## 4.1 Detailed Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"ðŸ“Š Test Set Performance:\")\n",
    "print(f\"\\n   Accuracy:  {accuracy*100:.2f}%\")\n",
    "print(f\"   Precision: {precision:.4f}\")\n",
    "print(f\"   Recall:    {recall:.4f}\")\n",
    "print(f\"   F1-Score:  {f1:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ What these metrics mean:\")\n",
    "print(f\"   â€¢ Accuracy:  {accuracy*100:.1f}% of predictions are correct\")\n",
    "print(f\"   â€¢ Precision: When model predicts POSITIVE, it's right {precision*100:.1f}% of the time\")\n",
    "print(f\"   â€¢ Recall:    Model finds {recall*100:.1f}% of all positive reviews\")\n",
    "print(f\"   â€¢ F1-Score:  Harmonic mean of precision and recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Confusion Matrix Breakdown:\")\n",
    "print(f\"   True Negatives:  {cm[0,0]} (correctly predicted negative)\")\n",
    "print(f\"   False Positives: {cm[0,1]} (predicted positive, actually negative)\")\n",
    "print(f\"   False Negatives: {cm[1,0]} (predicted negative, actually positive)\")\n",
    "print(f\"   True Positives:  {cm[1,1]} (correctly predicted positive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "test_reviews = [all_reviews[i] for i, label in enumerate(labels) if i < len(all_reviews)]\n",
    "misclassified_indices = np.where(y_pred.flatten() != y_test.flatten())[0]\n",
    "\n",
    "print(f\"ðŸ” Error Analysis: {len(misclassified_indices)} misclassified reviews\\n\")\n",
    "\n",
    "# Show first 5 errors\n",
    "for i, idx in enumerate(misclassified_indices[:5]):\n",
    "    true_label = \"POSITIVE\" if y_test[idx] == 1 else \"NEGATIVE\"\n",
    "    pred_label = \"POSITIVE\" if y_pred[idx] == 1 else \"NEGATIVE\"\n",
    "    prob = model.forward(X_test[idx:idx+1])[0, 0]\n",
    "    \n",
    "    print(f\"Error #{i+1}:\")\n",
    "    print(f\"   Review: [Sample from test set]\")\n",
    "    print(f\"   True Label: {true_label}\")\n",
    "    print(f\"   Predicted: {pred_label} (probability: {prob:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Reflection Questions\n",
    "\n",
    "## Question 1: Connection to MNIST (3 points)\n",
    "\n",
    "**Answer the following in the markdown cell below:**\n",
    "\n",
    "a) How is this text classification problem similar to MNIST digit classification?  \n",
    "b) What are the key differences?  \n",
    "c) What parts of the code stayed exactly the same?\n",
    "\n",
    "---\n",
    "\n",
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "(Write 3-5 sentences)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Understanding Bag-of-Words (4 points)\n",
    "\n",
    "**Answer the following:**\n",
    "\n",
    "a) Explain in your own words: What is the Bag-of-Words representation?  \n",
    "b) Why do we limit the vocabulary to the most common words instead of using ALL words?  \n",
    "c) What information do we LOSE when converting text to Bag-of-Words?  \n",
    "d) How does the hidden layer help the network make better decisions?\n",
    "\n",
    "---\n",
    "\n",
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "(Write 4-6 sentences)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Model Performance (3 points)\n",
    "\n",
    "**Analyze your results:**\n",
    "\n",
    "a) What was your final test accuracy? Is it good? Why or why not?  \n",
    "b) Did your model overfit? (Compare training vs test accuracy)  \n",
    "c) Looking at the misclassified examples, why do you think the model made mistakes?\n",
    "\n",
    "---\n",
    "\n",
    "**YOUR ANSWER HERE:**\n",
    "\n",
    "(Write 3-5 sentences)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Future Improvements (Optional - Bonus 3 points)\n",
    "\n",
    "**Think like a data scientist:**\n",
    "\n",
    "If you had more time, what would you try to improve the model? List 3 specific ideas and explain why each might help.\n",
    "\n",
    "---\n",
    "\n",
    "**YOUR IDEAS:**\n",
    "\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Submission Checklist\n",
    "\n",
    "Before submitting, make sure:\n",
    "\n",
    "- [ ] All code cells run without errors (Run â†’ Run All Cells)\n",
    "- [ ] All TODO sections are implemented (forward, compute_loss, training loop)\n",
    "- [ ] All plots are displayed\n",
    "- [ ] All reflection questions are answered\n",
    "- [ ] Final test accuracy is reported\n",
    "- [ ] File is named correctly: `LastName_FirstName_TextClassification.ipynb`\n",
    "\n",
    "**Good luck! ðŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
