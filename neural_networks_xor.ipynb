{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Lesson 1A: XOR Neural Network\n",
    "\n",
    "## Building Your First Neural Network from Scratch\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand why XOR is a classic neural network problem\n",
    "- Build a simple 2-layer neural network from scratch\n",
    "- Visualize forward propagation\n",
    "- Train the network and watch weights evolve\n",
    "- See how a trained network solves XOR\n",
    "\n",
    "**Duration:** ~45 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Why XOR?\n",
    "\n",
    "The XOR (exclusive OR) problem is historically important because it proved that single-layer perceptrons cannot solve it. This led to the development of multi-layer neural networks!\n",
    "\n",
    "**XOR Truth Table:**\n",
    "\n",
    "| Input A | Input B | Output |\n",
    "|---------|---------|--------|\n",
    "| 0       | 0       | 0      |\n",
    "| 0       | 1       | 1      |\n",
    "| 1       | 0       | 1      |\n",
    "| 1       | 1       | 0      |\n",
    "\n",
    "**The Challenge:** These points are not linearly separable! You cannot draw a single straight line to separate the 0s from the 1s.\n",
    "\n",
    "**The Solution:** A neural network with at least one hidden layer can learn this non-linear pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Install and import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Network Architecture\n",
    "\n",
    "We'll build a simple 2-layer network:\n",
    "\n",
    "```\n",
    "Input Layer (2 neurons)  ‚Üí  Hidden Layer (2 neurons)  ‚Üí  Output Layer (1 neuron)\n",
    "    [x1, x2]           ‚Üí      [h1, h2]              ‚Üí         [y]\n",
    "```\n",
    "\n",
    "**Layer Connections:**\n",
    "- Input ‚Üí Hidden: 2√ó2 = 4 weights + 2 biases\n",
    "- Hidden ‚Üí Output: 2√ó1 = 2 weights + 1 bias\n",
    "- **Total parameters:** 9 (4+2+2+1)\n",
    "\n",
    "**Activation Function:** We'll use sigmoid: œÉ(x) = 1 / (1 + e^(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivative of sigmoid for backpropagation\"\"\"\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Visualize the sigmoid function\n",
    "x = np.linspace(-6, 6, 100)\n",
    "y = sigmoid(x)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='Sigmoid(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Input (x)', fontsize=12)\n",
    "plt.ylabel('Output', fontsize=12)\n",
    "plt.title('Sigmoid Activation Function: œÉ(x) = 1/(1+e^-x)', fontsize=14, fontweight='bold')\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.5, label='Middle (0.5)')\n",
    "plt.axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Activation function defined\")\n",
    "print(\"\\nüîç Key Properties:\")\n",
    "print(f\"  ‚Ä¢ Output range: (0, 1)\")\n",
    "print(f\"  ‚Ä¢ Sigmoid(0) = {sigmoid(0):.3f}\")\n",
    "print(f\"  ‚Ä¢ Sigmoid(-5) ‚âà {sigmoid(-5):.3f} (almost 0)\")\n",
    "print(f\"  ‚Ä¢ Sigmoid(5) ‚âà {sigmoid(5):.3f} (almost 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Initialize the Neural Network\n",
    "\n",
    "Now let's create our network with random initial weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size=2, hidden_size=2, output_size=1):\n",
    "        \"\"\"Initialize a simple 2-layer neural network\"\"\"\n",
    "        # Weights and biases for input ‚Üí hidden layer\n",
    "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
    "        self.bias_hidden = np.random.randn(1, hidden_size)\n",
    "        \n",
    "        # Weights and biases for hidden ‚Üí output layer\n",
    "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
    "        self.bias_output = np.random.randn(1, output_size)\n",
    "        \n",
    "        # Store training history\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation through the network\"\"\"\n",
    "        # Input ‚Üí Hidden layer\n",
    "        self.hidden_input = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
    "        self.hidden_output = sigmoid(self.hidden_input)\n",
    "        \n",
    "        # Hidden ‚Üí Output layer\n",
    "        self.output_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
    "        self.output = sigmoid(self.output_input)\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, X, y, learning_rate=0.5):\n",
    "        \"\"\"Backpropagation to update weights\"\"\"\n",
    "        # Calculate output layer error\n",
    "        output_error = y - self.output\n",
    "        output_delta = output_error * sigmoid_derivative(self.output)\n",
    "        \n",
    "        # Calculate hidden layer error\n",
    "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
    "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights_hidden_output += self.hidden_output.T.dot(output_delta) * learning_rate\n",
    "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
    "        self.weights_input_hidden += X.T.dot(hidden_delta) * learning_rate\n",
    "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
    "    \n",
    "    def train(self, X, y, epochs=10000, print_every=1000):\n",
    "        \"\"\"Train the network\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            # Calculate loss (Mean Squared Error)\n",
    "            loss = np.mean((y - output) ** 2)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.backward(X, y)\n",
    "            \n",
    "            # Print progress\n",
    "            if epoch % print_every == 0:\n",
    "                print(f\"Epoch {epoch:5d} | Loss: {loss:.6f}\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Training completed! Final loss: {loss:.6f}\")\n",
    "\n",
    "# Create the network\n",
    "nn = SimpleNeuralNetwork(input_size=2, hidden_size=2, output_size=1)\n",
    "\n",
    "print(\"üß† Neural Network Initialized\")\n",
    "print(f\"\\nInitial Weights (Input ‚Üí Hidden):\")\n",
    "print(nn.weights_input_hidden)\n",
    "print(f\"\\nInitial Weights (Hidden ‚Üí Output):\")\n",
    "print(nn.weights_hidden_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Prepare Training Data\n",
    "\n",
    "Let's prepare our XOR training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR training data\n",
    "X_train = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "y_train = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])\n",
    "\n",
    "print(\"üìä XOR Training Data:\")\n",
    "print(\"=\"*40)\n",
    "for i in range(len(X_train)):\n",
    "    print(f\"Input: {X_train[i]} ‚Üí Target Output: {y_train[i][0]}\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Visualize the XOR problem\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red' if y == 0 else 'blue' for y in y_train]\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=colors, s=200, alpha=0.6, edgecolors='black', linewidth=2)\n",
    "\n",
    "for i, (x, y) in enumerate(X_train):\n",
    "    plt.annotate(f'({x},{y})‚Üí{y_train[i][0]}', \n",
    "                xy=(x, y), \n",
    "                xytext=(10, 10), \n",
    "                textcoords='offset points',\n",
    "                fontsize=12,\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.xlabel('Input A', fontsize=14)\n",
    "plt.ylabel('Input B', fontsize=14)\n",
    "plt.title('XOR Problem Visualization\\nüî¥ Red = Output 0 | üîµ Blue = Output 1', fontsize=16, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ùì Challenge: Can you draw a single straight line to separate red from blue?\")\n",
    "print(\"   (Hint: No! That's why we need a neural network!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Train the Network! üöÄ\n",
    "\n",
    "Now let's train our neural network to solve XOR. Watch the loss decrease as the network learns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "print(\"üèãÔ∏è Training the neural network...\\n\")\n",
    "nn.train(X_train, y_train, epochs=10000, print_every=2000)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(nn.loss_history, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(nn.loss_history, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training Loss (Log Scale)', fontsize=14, fontweight='bold')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìâ Loss decreased from {:.6f} to {:.6f}\".format(\n",
    "    nn.loss_history[0], nn.loss_history[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Test the Trained Network\n",
    "\n",
    "Let's see how well our network learned XOR!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network\n",
    "predictions = nn.forward(X_train)\n",
    "\n",
    "print(\"üéØ Neural Network Predictions:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Input A':<10} {'Input B':<10} {'Target':<10} {'Prediction':<15} {'Correct?'}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    input_a, input_b = X_train[i]\n",
    "    target = y_train[i][0]\n",
    "    prediction = predictions[i][0]\n",
    "    rounded_pred = round(prediction)\n",
    "    is_correct = \"‚úÖ\" if rounded_pred == target else \"‚ùå\"\n",
    "    \n",
    "    print(f\"{input_a:<10} {input_b:<10} {target:<10} {prediction:<15.4f} {is_correct}\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate accuracy\n",
    "rounded_predictions = np.round(predictions)\n",
    "accuracy = np.mean(rounded_predictions == y_train) * 100\n",
    "print(f\"\\nüéâ Accuracy: {accuracy:.1f}%\")\n",
    "\n",
    "if accuracy == 100:\n",
    "    print(\"\\nüèÜ Perfect! The network successfully learned XOR!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  The network needs more training or architecture adjustment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Visualize the Trained Network\n",
    "\n",
    "Let's visualize what the network learned by looking at the final weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß† Trained Network Architecture:\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LAYER 1: Input ‚Üí Hidden\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nWeights:\")\n",
    "print(nn.weights_input_hidden)\n",
    "print(\"\\nBiases:\")\n",
    "print(nn.bias_hidden)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LAYER 2: Hidden ‚Üí Output\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nWeights:\")\n",
    "print(nn.weights_hidden_output)\n",
    "print(\"\\nBias:\")\n",
    "print(nn.bias_output)\n",
    "\n",
    "# Visualize the decision boundary\n",
    "def plot_decision_boundary(nn, X, y):\n",
    "    \"\"\"Plot the decision boundary learned by the network\"\"\"\n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = -0.5, 1.5\n",
    "    y_min, y_max = -0.5, 1.5\n",
    "    h = 0.01\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Make predictions for each point in the mesh\n",
    "    Z = nn.forward(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, levels=20, cmap='RdBu', alpha=0.6)\n",
    "    plt.colorbar(label='Network Output')\n",
    "    \n",
    "    # Plot training points\n",
    "    colors = ['red' if label == 0 else 'blue' for label in y.flatten()]\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=colors, s=200, edgecolors='black', linewidth=2, zorder=5)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, (x, y_coord) in enumerate(X):\n",
    "        plt.annotate(f'{y[i][0]}', \n",
    "                    xy=(x, y_coord), \n",
    "                    ha='center', \n",
    "                    va='center',\n",
    "                    fontsize=14,\n",
    "                    fontweight='bold',\n",
    "                    color='white')\n",
    "    \n",
    "    plt.xlabel('Input A', fontsize=14)\n",
    "    plt.ylabel('Input B', fontsize=14)\n",
    "    plt.title('Decision Boundary Learned by Neural Network\\nüî¥ Red regions ‚Üí 0 | üîµ Blue regions ‚Üí 1', \n",
    "             fontsize=16, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(nn, X_train, y_train)\n",
    "\n",
    "print(\"\\n‚ú® The network created a non-linear decision boundary!\")\n",
    "print(\"   This is what makes neural networks powerful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Understanding What Happened\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "1. **Non-linearity is crucial:** The sigmoid activation function allows the network to create curved decision boundaries.\n",
    "\n",
    "2. **Hidden layers enable complexity:** The 2 hidden neurons learned to represent the problem in a way that makes it linearly separable in their space.\n",
    "\n",
    "3. **Learning through gradients:** Backpropagation adjusted weights to minimize the error between predictions and targets.\n",
    "\n",
    "4. **Small networks can solve XOR:** We only needed 9 parameters (weights + biases) to solve this problem!\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "In **Lesson 1B**, we'll scale up to recognize handwritten digits using the MNIST dataset. You'll see how these same principles apply to real-world image classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Exercises (Optional)\n",
    "\n",
    "Try modifying the code to explore these questions:\n",
    "\n",
    "1. **Change the hidden layer size:** What happens with 3 or 4 hidden neurons? Does it train faster?\n",
    "\n",
    "2. **Adjust the learning rate:** Try values like 0.1, 1.0, or 2.0. What happens to training?\n",
    "\n",
    "3. **Different activation functions:** Can you implement ReLU instead of sigmoid?\n",
    "\n",
    "4. **Visualize intermediate steps:** Print the hidden layer activations for each input. What patterns do you see?\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** üéâ You've built and trained your first neural network from scratch!\n",
    "\n",
    "**Next:** Head to **Lesson 1B** to tackle handwritten digit recognition with MNIST."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
